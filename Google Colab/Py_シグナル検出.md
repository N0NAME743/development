
#2025-06-15è¿½åŠ å†…å®¹
##å£²ã‚Šç›®ç·šã‚·ã‚°ãƒŠãƒ«ã‚’è¿½åŠ 
##ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®ã‚¹ã‚­ãƒƒãƒ—æ©Ÿèƒ½ã‚’è¿½åŠ 

# ================================
# Sec1ï½œã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
# ================================

!pip install --upgrade gspread gspread_dataframe google-auth yfinance --quiet
!pip install ta --quiet

import os
import pandas as pd
import yfinance as yf
import gspread
from google.colab import auth
from gspread_dataframe import get_as_dataframe, set_with_dataframe
from googleapiclient.discovery import build
from googleapiclient.http import MediaInMemoryUpload
from google.auth.transport.requests import Request
from datetime import datetime, timezone, timedelta
import google.auth
from google.colab import drive
import time
from collections import defaultdict

pd.set_option('future.no_silent_downcasting', True)

# ================================
# Sec2ï½œå®šç¾©ãƒ»é–¢æ•°
# ================================

import numpy as np

SAVE_PATH = "drive/MyDrive/ColabNotebooks/éŠ˜æŸ„åˆ†æ/signal"  # ğŸ”§ ä¿å­˜å…ˆã®ãƒ‘ã‚¹ã‚’æ­£ã—ãè¨­å®š
BACKUP_SUBFOLDER = "backup"  # ğŸ”§ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã¯ã“ã®ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€ã«

# GoogleDriveé–¢æ•°
def mount_drive():
    drive.mount('/content/drive')

def authenticate_services():
    auth.authenticate_user()
    creds, _ = google.auth.default()
    return creds, gspread.authorize(creds), creds

def get_drive_folder_id_by_path(path, creds):
    service = build('drive', 'v3', credentials=creds)
    parts = path.strip("/").split("/")
    parent_id = 'root'
    for part in parts:
        query = f"name='{part}' and mimeType='application/vnd.google-apps.folder' and '{parent_id}' in parents and trashed=false"
        results = service.files().list(q=query, spaces='drive', fields='files(id, name)').execute()
        files = results.get('files', [])
        if files:
            parent_id = files[0]['id']
        else:
            file_metadata = {
                'name': part,
                'mimeType': 'application/vnd.google-apps.folder',
                'parents': [parent_id]
            }
            file = service.files().create(body=file_metadata, fields='id').execute()
            parent_id = file['id']
    return parent_id, service

def delete_existing_file_by_name(drive_service, folder_id, file_name):
    query = f"'{folder_id}' in parents and name = '{file_name}' and mimeType = 'application/vnd.google-apps.spreadsheet'"
    results = drive_service.files().list(q=query, fields="files(id, name)").execute()
    files = results.get('files', [])
    for file in files:
        drive_service.files().delete(fileId=file['id']).execute()

def create_spreadsheet_in_folder(title, folder_id, creds):
    drive_service = build("drive", "v3", credentials=creds)
    file_metadata = {
        "name": title,
        "mimeType": "application/vnd.google-apps.spreadsheet",
        "parents": [folder_id]
    }
    file = drive_service.files().create(body=file_metadata, fields="id").execute()
    return file["id"]

# æ—¥ä»˜ã®å–å¾—
def get_today_str():
    JST = timezone(timedelta(hours=9))
    now = datetime.now(JST)
    return now.strftime("%Y-%m-%d"), now.strftime("%Y-%m-%d %H:%M:%S")

# ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®Colum
def reorder_columns(df, preferred_order):
    existing_cols = [col for col in preferred_order if col in df.columns]
    other_cols = [col for col in df.columns if col not in existing_cols]
    return df[existing_cols + other_cols]

def symbol_to_num(symbol):
    # .Tã‚’é™¤å»ã—ã€æ•°å€¤éƒ¨åˆ†ã®ã¿æŠ½å‡º
    s = str(symbol).replace('.T', '')
    try:
        return int(float(s))
    except ValueError:
        return np.nan  # æ•°å€¤åŒ–ã§ããªã„å ´åˆã¯NaN

def clean_dataframe_columns(df, expected_cols):
    """
    DataFrameã‹ã‚‰é‡è¤‡åˆ—ï¼ˆä¾‹: 'MultiSign.1'ï¼‰ã‚’å–ã‚Šé™¤ãã€åˆ—é †ã‚’expected_colsã«æ•´ãˆã‚‹ã€‚
    """
    # 1. æœŸå¾…ã•ã‚Œã‚‹åˆ—åã«å¯¾ã—ã¦ã€.1 ã‚„ .2 ãŒã¤ã„ãŸé‡è¤‡åˆ—ãŒã‚ã‚Œã°å‰Šé™¤
    deduped_cols = {}
    for col in df.columns:
        base_col = col.split('.')[0]
        if base_col not in deduped_cols:
            deduped_cols[base_col] = col  # æœ€åˆã«å‡ºã¦ããŸæ­£è¦åã‚’æ¡ç”¨

    df = df[list(deduped_cols.values())]

    # 2. åˆ—é †ã‚’expected_colsã®é †ã«æ•´ãˆã‚‹ï¼ˆå­˜åœ¨ã™ã‚‹åˆ—ã®ã¿ï¼‰
    df = df[[col for col in expected_cols if col in df.columns]]

    return df

def normalize_symbol_column(df):
    """
    Symbolåˆ—ã‚’ "XXXX.T" å½¢å¼ã«çµ±ä¸€ã™ã‚‹
    - floatå½¢å¼ã® .0 é™¤å»
    - .T ã®é‡è¤‡é™¤å»
    - .0.T ã®èª¤å¤‰æ›é™¤å»
    """
    def clean_symbol(val):
        try:
            val = str(val).strip().replace('$', '')
            val = val.replace('.0.T', '')  # â† è¿½åŠ ãƒã‚¤ãƒ³ãƒˆ
            if val.endswith('.0'):
                val = val[:-2]
            val = val.replace('.T', '')  # é‡è¤‡é™¤å»
            return val + '.T'
        except:
            return ""

    if "Symbol" in df.columns:
        df["Symbol"] = df["Symbol"].apply(clean_symbol)
    return df

import yfinance as yf

# æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã¨ä¼šç¤¾åã‚’å–å¾—ã™ã‚‹é–¢æ•°
def get_stock_data(symbol):
    try:
        ticker = yf.Ticker(symbol)
        info = ticker.info
        name = info.get("shortName", symbol)

        df = yf.download(symbol, period="18mo", interval="1d", auto_adjust=False)

        if df.empty:
            raise ValueError("å–å¾—çµæœãŒç©ºã§ã™")

        if isinstance(df.columns, pd.MultiIndex):
            df.columns = df.columns.get_level_values(0)

        df = df.dropna(subset=["Open", "High", "Low", "Close", "Volume"]).astype(float)
        df.index.name = "Date"
        return df, name

    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {symbol} - {e}")
        return pd.DataFrame(), symbol

# ================================
# å®Œå…¨çµ±åˆã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆå®šç¾© + å‡ºåŠ›å¯¾å¿œï¼‰
# ================================

import pandas as pd
import numpy as np
from ta.trend import ADXIndicator, MACD
from ta.momentum import RSIIndicator

# ã‚¹ã‚³ã‚¢è¾æ›¸ï¼ˆæ³¨ç›®åº¦ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆãƒ»ç‚¹æ•°ï¼‰
LEVELS = {
    "â˜…â˜…â˜…": {
        "keywords": [
            {"text": "ä¸Šå ´æ¥é«˜å€¤çªç ´", "comment": "ä¸Šå ´æ¥é«˜å€¤ã‚’æ›´æ–°ã—æ³¨ç›®åº¦MAX"},
            {"text": "ç›´è¿‘é«˜å€¤çªç ´", "comment": "ç›´è¿‘é«˜å€¤ã‚’çªç ´ã—ã€ãƒˆãƒ¬ãƒ³ãƒ‰ç¶™ç¶šã®å¯èƒ½æ€§"},
            {"text": "å‡ºæ¥é«˜æ€¥å¢—", "comment": "å‡ºæ¥é«˜ãŒæ€¥å¢—ã—ã€å¤§å£ã®è²·ã„ãŒå…¥ã£ã¦ã„ã‚‹å¯èƒ½æ€§"},
            {"text": "æœŸé–“å†…é«˜å€¤æ›´æ–°ï¼ˆè¦æ³¨ç›®ï¼‰", "comment": "å–å¾—å¯èƒ½æœŸé–“å†…ã®æœ€é«˜å€¤ã‚’æ›´æ–°ã—ã¦ãŠã‚Šã€æ³¨ç›®åº¦ã¯é«˜ã„ã§ã™ã€‚"},
            {"text": "é€±è¶³é«˜å€¤ãƒ–ãƒ¬ã‚¤ã‚¯", "comment": "é€±è¶³ã§ã‚‚é«˜å€¤ã‚’æ›´æ–°ã—ã¦ãŠã‚Šã€ä¸­é•·æœŸçš„ãªå¼·ã•ãŒä¼ºãˆã¾ã™ã€‚"},
            {"text": "MACDé€±è¶³é™½è»¢", "comment": "é€±è¶³ãƒ™ãƒ¼ã‚¹ã®MACDãŒé™½è»¢ã—ã€ä¸­é•·æœŸã§è²·ã„ã®å‹¢ã„ãŒå‡ºã¦ã„ã¾ã™ã€‚"}
        ],
        "score": 3
    },
    "â˜…â˜…": {
        "keywords": [
            {"text": "MACDé™½è»¢", "comment": "MACDãŒé™½è»¢ã—ã€è²·ã„ã®å‹¢ã„ãŒå‡ºå§‹ã‚ã¦ã„ã¾ã™ã€‚"},
            {"text": "çŸ­æœŸã‚´ãƒ¼ãƒ«ãƒ‡ãƒ³ã‚¯ãƒ­ã‚¹", "comment": "çŸ­æœŸã®ç§»å‹•å¹³å‡ç·šãŒã‚´ãƒ¼ãƒ«ãƒ‡ãƒ³ã‚¯ãƒ­ã‚¹ã‚’å½¢æˆã—ã¦ã„ã¾ã™ã€‚"},
            {"text": "ä¸­æœŸã‚´ãƒ¼ãƒ«ãƒ‡ãƒ³ã‚¯ãƒ­ã‚¹", "comment": "ä¸­æœŸã®ç§»å‹•å¹³å‡ç·šã‚‚ã‚´ãƒ¼ãƒ«ãƒ‡ãƒ³ã‚¯ãƒ­ã‚¹ã—ã€ãƒˆãƒ¬ãƒ³ãƒ‰ã®æŒç¶šæ€§ãŒé«˜ã¾ã£ã¦ã„ã¾ã™ã€‚"},
            {"text": "RSIåç™º", "comment": "RSIãŒåç™ºã—ã€åè»¢ä¸Šæ˜‡ã®å…†ã—ãŒã‚ã‚Šã¾ã™ã€‚"},
            {"text": "å‡ºæ¥é«˜â†‘", "comment": "å‡ºæ¥é«˜ãŒå¢—ãˆã¦ãŠã‚Šæ³¨ç›®ãŒé›†ã¾ã£ã¦ã„ã¾ã™ã€‚"},
            {"text": "ä¸‰é€£ç¶šé™½ç·š", "comment": "3æ—¥é€£ç¶šã®é™½ç·šãŒç¶šã„ã¦ãŠã‚Šã€å¼·ã„ä¸Šæ˜‡åœ§åŠ›ãŒç¢ºèªã§ãã¾ã™ã€‚"},
            {"text": "é™½ã®ä¸¸åŠä¸»", "comment": "é™½ã®ä¸¸åŠä¸»ãŒå‡ºç¾ã—ã€è²·ã„åœ§åŠ›ã®å¼·ã•ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚"},
            {"text": "ä¸‹å€¤åˆ‡ã‚Šä¸Šã’ç¶™ç¶š", "comment": "å®‰å€¤ãŒåˆ‡ã‚Šä¸ŠãŒã‚‹å±•é–‹ãŒç¶šã„ã¦ãŠã‚Šã€ä¸‹æ”¯ãˆã®å¼·ã•ãŒè¦‹ã‚‰ã‚Œã¾ã™ã€‚"}
        ],
        "score": 2
    },
    "â˜…": {
        "keywords": [
            {"text": "çª“é–‹ã‘ä¸Šæ˜‡", "comment": "çª“ã‚’é–‹ã‘ã¦ä¸Šæ˜‡ã—å‹¢ã„ã‚ã‚Š"},
            {"text": "ä¸‹é«­é™½ç·š", "comment": "ä¸‹é«­é™½ç·šãŒå‡ºç¾ã—ã€æŠ¼ã—ç›®è²·ã„ã®å¯èƒ½æ€§ã‚ã‚Š"},
            {"text": "æŠ¼ã—ç›®é™½ç·š", "comment": "èª¿æ•´å¾Œã®æŠ¼ã—ç›®ã§é™½ç·šãŒå‡ºã¦ãŠã‚Šã€åç™ºã®å…†å€™ãŒè¦‹ã‚‰ã‚Œã¾ã™ã€‚"},
            {"text": "ä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³åç™º", "comment": "ä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ä»˜è¿‘ã§åç™ºã—ã¦ãŠã‚Šã€ä¸‹å€¤æ”¯æŒãŒç¢ºèªã§ãã¾ã™ã€‚"}
        ],
        "score": 1
    }
}
LEVELS_SELL = {
    "â–¼â–¼â–¼": {
        "keywords": [
            {"text": "MACDé™°è»¢", "comment": "MACDãŒé™°è»¢ã—ã€ä¸‹è½ãƒˆãƒ¬ãƒ³ãƒ‰å…¥ã‚Šã®å…†ã—ãŒã‚ã‚Šã¾ã™ã€‚"},
            {"text": "çŸ­æœŸãƒ‡ãƒƒãƒ‰ã‚¯ãƒ­ã‚¹", "comment": "çŸ­æœŸã®ç§»å‹•å¹³å‡ç·šãŒãƒ‡ãƒƒãƒ‰ã‚¯ãƒ­ã‚¹ã‚’å½¢æˆã—ã¦ã„ã¾ã™ã€‚"},
            {"text": "ä¸Šé«­é™°ç·š", "comment": "ä¸Šé«­ãŒé•·ã„é™°ç·šãŒå‡ºç¾ã—ã€ä¸Šå€¤ã®é‡ã•ã‚’ç¤ºå”†ã—ã¦ã„ã¾ã™ã€‚"},
            {"text": "é™°ã®ä¸¸åŠä¸»", "comment": "é™°ã®ä¸¸åŠä¸»ãŒå‡ºç¾ã—ã€å£²ã‚Šåœ§åŠ›ãŒå¼·ã¾ã£ã¦ã„ã‚‹å…†ã—ã§ã™ã€‚"},
        ],
        "score": -3
    },
    "â–¼â–¼": {
        "keywords": [
            {"text": "RSIéç†±", "comment": "RSIãŒé«˜å€¤åœã«ã‚ã‚Šã€éç†±æ„ŸãŒå‡ºã¦ã„ã¾ã™ã€‚"},
            {"text": "ä¸‹è½ãƒˆãƒ¬ãƒ³ãƒ‰ç¶™ç¶š", "comment": "å®‰å€¤ãŒåˆ‡ã‚Šä¸‹ãŒã‚‹å½¢ã¨ãªã£ã¦ãŠã‚Šã€ä¸‹é™ãƒˆãƒ¬ãƒ³ãƒ‰ãŒç¶šã„ã¦ã„ã¾ã™ã€‚"},
            {"text": "ç§»å‹•å¹³å‡ç·šä¸‹æŠœã‘", "comment": "é‡è¦ãªç§»å‹•å¹³å‡ç·šã‚’ä¸‹å›ã£ã¦ãŠã‚Šã€ä¸‹è½è­¦æˆ’ã§ã™ã€‚"},
        ],
        "score": -2
    },
    "â–¼": {
        "keywords": [
            {"text": "å‡ºæ¥é«˜æ¸›å°‘", "comment": "å‡ºæ¥é«˜ãŒæ¸›å°‘ã—ã¦ãŠã‚Šã€ä¸Šæ˜‡ã®å‹¢ã„ãŒéˆã£ã¦ã„ã¾ã™ã€‚"},
            {"text": "é€£ç¶šé™°ç·š", "comment": "é€£ç¶šã—ãŸé™°ç·šãŒç¶šã„ã¦ãŠã‚Šã€å£²ã‚Šå„ªå‹¢ã§ã™ã€‚"},
        ],
        "score": -1
    }
}

def adjust_attention_by_warning(attention, comment):
    """
    è­¦æˆ’ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚Œã°ã€æ³¨ç›®åº¦ã‚’â˜…â†’â˜†ã«1æ®µéšå¤‰æ›ã™ã‚‹
    """
    warning_keywords = ["ä»•æ‰‹", "éç†±", "æ³¨æ„", "è­¦æˆ’", "ä¿¡ç”¨å€ç‡é«˜", "ä¸Šé«­é™°ç·š", "è¶…å°å‹æ ª"]
    if any(k in comment for k in warning_keywords):
        if attention == "â˜…â˜…â˜…":
            return "â˜…â˜…â˜†"
        elif attention == "â˜…â˜…":
            return "â˜…â˜†"
        elif attention == "â˜…":
            return "â˜†"
    return attention

# ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆï¼†è©•ä¾¡é–¢æ•°

import random

def get_summary_by_attention(attention, buy_count=0, sell_count=0):
    if buy_count >= 2 and sell_count == 0:
        return "â¡ ç·è©•ï¼šè²·ã„ã‚·ã‚°ãƒŠãƒ«ãŒå„ªå‹¢ã§ã€ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚’æ¤œè¨ã§ãã‚‹å ´é¢ã§ã™ã€‚"
    elif sell_count >= 2 and buy_count == 0:
        return "â¡ ç·è©•ï¼šä¸‹è½ã‚·ã‚°ãƒŠãƒ«ãŒå„ªå‹¢ã§ã€åˆ©ç¢ºã‚„è­¦æˆ’ãŒæ¨å¥¨ã•ã‚Œã‚‹å±€é¢ã§ã™ã€‚"
    elif buy_count >= 2 and sell_count >= 2:
        return "â¡ ç·è©•ï¼šè²·ã„ã¨å£²ã‚Šã‚·ã‚°ãƒŠãƒ«ãŒæ‹®æŠ—ã—ã¦ãŠã‚Šã€æ§˜å­è¦‹ãŒå¦¥å½“ã§ã™ã€‚"
    elif attention in ["â˜…â˜…â˜…", "â˜…â˜…"]:
        return "â¡ ç·è©•ï¼šä¸€å®šã®ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«å¥½è»¢ãŒè¦‹ã‚‰ã‚Œã¾ã™ãŒã€éç†±æ„Ÿã«ã‚‚æ³¨æ„ãŒå¿…è¦ã§ã™ã€‚"
    elif attention == "â˜…":
        return "â¡ ç·è©•ï¼šåè»¢ã®å…†ã—ã¯ã‚ã‚‹ã‚‚ã®ã®ã€åˆ¤æ–­ã¯æ…é‡ã«ã€‚"
    else:
        return "â¡ ç·è©•ï¼šä»Šã®ã¨ã“ã‚æ˜ç¢ºãªãƒˆãƒ¬ãƒ³ãƒ‰ã¯ç¢ºèªã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"

def analyze_signals(signals, adx_value=None):
    total_score = 0
    buy_count = 0
    sell_count = 0
    comments = []

    # é‡è¤‡ã‚·ã‚°ãƒŠãƒ«ã®é™¤å»
    if "å‡ºæ¥é«˜â†‘" in signals and "å‡ºæ¥é«˜æ€¥å¢—" in signals:
        signals = [s for s in signals if s != "å‡ºæ¥é«˜â†‘"]

    for level in list(LEVELS.values()) + list(LEVELS_SELL.values()):
        for item in level["keywords"]:
            if item["text"] in signals:
                score = level["score"]
                total_score += score
                if score > 0:
                    buy_count += 1
                    comments.append(f"âœ… {item['comment']}")
                else:
                    sell_count += 1
                    comments.append(f"âš ï¸ {item['comment']}")

    # ADXã®å½±éŸ¿ï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰ã®å¼·ã•ï¼‰
    if adx_value:
        if adx_value >= 40:
            total_score += 1
            comments.append("â˜…å¼·ã„ãƒˆãƒ¬ãƒ³ãƒ‰ãŒç¢ºèªã•ã‚Œã¦ãŠã‚Šã€é †å¼µã‚Šã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã¨ã—ã¦æœ‰åŠ¹ã§ã™ã€‚")
        elif adx_value >= 25:
            comments.append("â˜†ç¾åœ¨ã¯ä¸­ç¨‹åº¦ã®ãƒˆãƒ¬ãƒ³ãƒ‰ãŒç™ºç”Ÿã—ã¦ãŠã‚Šã€æ–¹å‘æ„ŸãŒå‡ºå§‹ã‚ã¦ã„ã¾ã™ã€‚")

    # æ³¨ç›®åº¦ã®ç®—å‡º
    if total_score >= 6:
        attention = "â˜…â˜…â˜…"
    elif total_score >= 3:
        attention = "â˜…â˜…"
    elif total_score >= 1:
        attention = "â˜…"
    else:
        attention = "ãƒ¼"

    # ç·è©•ã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆï¼ˆbuy/sellã‚«ã‚¦ãƒ³ãƒˆã«å¿œã˜ã¦å¤‰åŒ–ï¼‰
    summary = get_summary_by_attention(attention, buy_count, sell_count)
    full_comment = "\n".join(comments + [summary])

    score_str = f"{total_score:.1f} / {max(total_score, 10):.1f}"

    return attention, full_comment, total_score, score_str

# ã‚³ãƒ¡ãƒ³ãƒˆå†…å®¹ã‹ã‚‰ã‚¢ã‚¯ã‚·ãƒ§ãƒ³åˆ¤å®šï¼ˆæ³¨ç›®åº¦ãƒ™ãƒ¼ã‚¹ï¼‹è­¦å‘Šè£œæ­£ã‚ã‚Šï¼‰
def judge_action_by_comment(comment, attention=None):
    buy_count = comment.count("âœ…")
    sell_count = comment.count("âš ï¸")

    if sell_count >= 2 and buy_count == 0:
        return "å£²ã‚Šï¼ˆåˆ©ç¢º/è­¦æˆ’ï¼‰"
    elif buy_count >= 2 and sell_count == 0:
        return "è²·ã„ï¼ˆã‚¨ãƒ³ãƒˆãƒªãƒ¼æ¤œè¨ï¼‰"
    elif buy_count >= 2 and sell_count >= 2:
        return "ä¸­ç«‹ï¼ˆã‚·ã‚°ãƒŠãƒ«æ‹®æŠ—ï¼‰"
    elif attention in ["â˜…â˜…â˜…", "â˜…â˜…â˜†"]:
        return "è²·ã„ï¼ˆã‚¨ãƒ³ãƒˆãƒªãƒ¼æ¤œè¨ï¼‰"
    elif attention in ["â˜…â˜…", "â˜…â˜†"]:
        return "è²·ã„ï¼ˆã‚¨ãƒ³ãƒˆãƒªãƒ¼æ¤œè¨ï¼‰"
    elif attention in ["â˜…", "â˜†"]:
        return "ä¸­ç«‹ï¼ˆæ§˜å­è¦‹ï¼‰"
    else:
        return "ä¸­ç«‹ï¼ˆæ§˜å­è¦‹ï¼‰"

# ãƒ­ãƒ¼ã‚½ã‚¯è¶³ãƒ‘ã‚¿ãƒ¼ãƒ³ç°¡æ˜“æ¤œå‡ºï¼ˆä»»æ„è¿½åŠ å¯èƒ½ï¼‰
def detect_candlestick_patterns(df):
    patterns = []
    open_, close, high, low = df.iloc[-1][["Open", "Close", "High", "Low"]]
    body = abs(close - open_)
    lower_shadow = min(open_, close) - low
    upper_shadow = high - max(open_, close)
    if close > open_ and lower_shadow > body * 2:
        patterns.append("ä¸‹é«­é™½ç·š")
    if close > open_ and upper_shadow < body * 0.1 and lower_shadow < body * 0.1:
        patterns.append("é™½ã®ä¸¸åŠä¸»")
    return patterns

# === é€±è¶³ã‚·ã‚°ãƒŠãƒ«æ¤œå‡º ===
def detect_weekly_signals(df):
    signals = []
    df_weekly = df.resample("W-FRI").last().dropna()
    if len(df_weekly) < 20:
        return signals

    # é€±è¶³é«˜å€¤ãƒ–ãƒ¬ã‚¤ã‚¯
    if df_weekly["Close"].iloc[-1] > df_weekly["High"].iloc[-20:-1].max():
        signals.append("é€±è¶³é«˜å€¤ãƒ–ãƒ¬ã‚¤ã‚¯")

    # MACDé€±è¶³é™½è»¢
    macd = MACD(df_weekly["Close"])
    macd_line = macd.macd()
    macd_signal = macd.macd_signal()
    if macd_line.iloc[-2] < macd_signal.iloc[-2] and macd_line.iloc[-1] > macd_signal.iloc[-1]:
        signals.append("MACDé€±è¶³é™½è»¢")

    return signals

# === ä¸­æœŸGCæ¤œå‡º ===
def detect_mid_term_golden_cross(df):
    if len(df) < 75:
        return []
    ma25 = df["Close"].rolling(window=25).mean()
    ma75 = df["Close"].rolling(window=75).mean()
    cross = (ma25.shift(1) < ma75.shift(1)) & (ma25 > ma75)
    return ["ä¸­æœŸã‚´ãƒ¼ãƒ«ãƒ‡ãƒ³ã‚¯ãƒ­ã‚¹"] if cross.iloc[-1] else []

# === æŠ¼ã—ç›®é™½ç·šæ¤œå‡º ===
def detect_pullback_bounce(df):
    if len(df) < 3:
        return []
    high_break = df["Close"].iloc[-3] > df["High"].iloc[-20:-4].max()
    pullback = df["Close"].iloc[-2] < df["Close"].iloc[-3]
    rebound = df["Close"].iloc[-1] > df["Open"].iloc[-1]
    if high_break and pullback and rebound:
        return ["æŠ¼ã—ç›®é™½ç·š"]
    return []

# === ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³åç™ºæ¤œå‡º ===
def detect_trendline_bounce(df):
    if len(df) < 10:
        return []
    lows = df["Low"].iloc[-10:]
    x = np.arange(len(lows))
    coef = np.polyfit(x, lows, 1)
    trendline = coef[0] * x + coef[1]
    if lows.iloc[-1] > trendline[-1]:
        return ["ä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³åç™º"]
    return []

# === ä¸‹å€¤åˆ‡ã‚Šä¸Šã’æ¤œå‡º ===
def detect_higher_lows(df):
    if len(df) < 3:
        return []
    lows = df["Low"].iloc[-3:]
    if lows.iloc[0] < lows.iloc[1] < lows.iloc[2]:
        return ["ä¸‹å€¤åˆ‡ã‚Šä¸Šã’ç¶™ç¶š"]
    return []

# âœ… analyze_stock() é–¢æ•°ï¼ˆä»•æ‰‹æ ªãƒªã‚¹ã‚¯ï¼‹é«˜å€¤çªç ´ä¿®æ­£æ¸ˆã¿ï¼‰
def analyze_stock(df, info=None):
    if len(df) < 30:
        return [], "ãƒ¼", "", "ä¸­ç«‹ï¼ˆæ§˜å­è¦‹ï¼‰", 0, ""

    df = df.copy()
    close = df["Close"]
    high = df["High"]
    low = df["Low"]
    open_ = df["Open"]
    volume = df["Volume"]

    # ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™
    df["RSI"] = RSIIndicator(close, window=14).rsi()
    macd = MACD(close)
    df["MACD"] = macd.macd()
    df["MACD_signal"] = macd.macd_signal()
    adx = ADXIndicator(high, low, close, window=14)
    df["ADX"] = adx.adx()

    signals = []

    # ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ã‚·ã‚°ãƒŠãƒ«
    ##è²·ã„ç›®ç·š
    if df["MACD"].iloc[-2] < df["MACD_signal"].iloc[-2] and df["MACD"].iloc[-1] > df["MACD_signal"].iloc[-1]:
        signals.append("MACDé™½è»¢")
    if df["RSI"].iloc[-2] < 30 and df["RSI"].iloc[-1] > 30:
        signals.append("RSIåç™º")
    if volume.iloc[-1] > volume.rolling(5).mean().iloc[-2] * 1.5:
        signals.append("å‡ºæ¥é«˜â†‘")
    if volume.iloc[-1] > volume.rolling(5).mean().iloc[-2] * 2.0:
        signals.append("å‡ºæ¥é«˜æ€¥å¢—")
    ##å£²ã‚Šç›®ç·š
    # MACDé™°è»¢ï¼ˆã™ã§ã«ã‚ã‚‹ï¼‰
    if df["MACD"].iloc[-2] > df["MACD_signal"].iloc[-2] and df["MACD"].iloc[-1] < df["MACD_signal"].iloc[-1]:
        signals.append("MACDé™°è»¢")
    # RSIéç†±ï¼ˆ70è¶…ãˆï¼‰
    if df["RSI"].iloc[-2] < 70 and df["RSI"].iloc[-1] > 70:
        signals.append("RSIéç†±")
    # ç§»å‹•å¹³å‡ç·šä¸‹æŠœã‘ï¼ˆæ–°è¦è¿½åŠ å¯ï¼‰
    if df["Close"].iloc[-1] < df["Close"].rolling(window=25).mean().iloc[-1]:
        signals.append("ç§»å‹•å¹³å‡ç·šä¸‹æŠœã‘")

    # ç›´è¿‘60å–¶æ¥­æ—¥é«˜å€¤ãƒ–ãƒ¬ã‚¤ã‚¯
    recent_high = df["High"].iloc[-60:-1].max()
    if close.iloc[-1] > recent_high:
        signals.append("ç›´è¿‘é«˜å€¤çªç ´")

    # Highã‚’å«ã‚€æœŸé–“å†…é«˜å€¤åˆ¤å®š
    period_high = df["High"].max()
    if max(close.iloc[-1], high.iloc[-1]) > period_high:
        signals.append("æœŸé–“å†…é«˜å€¤æ›´æ–°ï¼ˆè¦æ³¨ç›®ï¼‰")

    # ãƒ­ãƒ¼ã‚½ã‚¯è¶³ãƒ»å±é™ºãƒ‘ã‚¿ãƒ¼ãƒ³
    signals += detect_candlestick_patterns(df)
    signals += detect_danger_signals(df)

    # æ€¥é¨°å±¥æ­´
    spike_flag = detect_spike_history(df, threshold=0.4)
    if spike_flag:
        signals.append(spike_flag)

    # ãƒ•ã‚¡ãƒ³ãƒ€ãƒ¡ãƒ³ã‚¿ãƒ«è­¦å‘Š
    if info:
        signals += detect_risky_fundamentals(info)

    # æ–°è¦è¿½åŠ ã‚·ã‚°ãƒŠãƒ«ã®çµ±åˆ
    signals += detect_weekly_signals(df)
    signals += detect_mid_term_golden_cross(df)
    signals += detect_pullback_bounce(df)
    signals += detect_trendline_bounce(df)
    signals += detect_higher_lows(df)

    adx_last = df["ADX"].iloc[-1]
    attention, comment, score, score_str = analyze_signals(signals, adx_last)

    # â­ è­¦æˆ’ã‚³ãƒ¡ãƒ³ãƒˆã‚’ã‚‚ã¨ã«æ³¨ç›®åº¦ã‚’èª¿æ•´
    adjusted_attention = adjust_attention_by_warning(attention, comment)
    action = judge_action_by_comment(comment, adjusted_attention)

    return signals, adjusted_attention, comment, action, score_str

# è¿½åŠ é–¢æ•°ç¾¤

def detect_danger_signals(df):
    signals = []
    open_, close, high, low = df.iloc[-1][["Open", "Close", "High", "Low"]]
    body = abs(close - open_)
    upper_shadow = high - max(open_, close)
    if upper_shadow > body * 2 and close < open_:
        signals.append("ä¸Šé«­é™°ç·šï¼ˆè­¦æˆ’ï¼‰")
    return signals

# ä¿®æ­£ç‰ˆï¼šæ€¥é¨°å±¥æ­´åˆ¤å®šï¼ˆ40%ä»¥ä¸Š + ç›´è¿‘60æ—¥é–“ï¼‰
def detect_spike_history(df, threshold=0.4):
    recent_df = df[-60:]
    max_close = recent_df["Close"].max()
    min_close = recent_df["Close"].min()
    if (max_close - min_close) / min_close >= threshold:
        return "æ€¥é¨°å±¥æ­´ã‚ã‚Šï¼ˆä»•æ‰‹æ³¨æ„ï¼‰"
    return ""

# ä¿®æ­£ç‰ˆï¼šæ™‚ä¾¡ç·é¡100å„„æœªæº€ã‚’è­¦å‘Šï¼ˆä¿¡ç”¨å€ç‡å«ã‚€ï¼‰
def detect_risky_fundamentals(info):
    warnings = []
    if info.get("marketCap", 1e12) < 10_000_000_000:
        warnings.append("è¶…å°å‹æ ªï¼ˆæ³¨æ„ï¼‰")
    if info.get("shortRatio", 0) > 3.0:
        warnings.append("ä¿¡ç”¨å€ç‡é«˜ï¼ˆéç†±æ„Ÿï¼‰")
    return warnings

def process_all_sheets(spreadsheet_name, gc, drive_service, folder_id_main, folder_id_backup, creds):
    sheet_process_logs = []
    global SAVE_PATH
    today_str, timestamp_str = get_today_str()
    spreadsheet = gc.open(spreadsheet_name)
    sheet_list = spreadsheet.worksheets()

    all_dfs = []  # å„ã‚·ãƒ¼ãƒˆã®DFã‚’æ ¼ç´

    for worksheet in sheet_list:
        start_time_sheet = time.time()
        sheet_name = worksheet.title
        print(f"\U0001f50d å‡¦ç†ä¸­: {sheet_name}")
        df = get_as_dataframe(worksheet, evaluate_formulas=True)

        if "Symbol" not in df.columns:
            print(f"âš ï¸ {sheet_name} ã«Symbolåˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            continue

        for col in ["Name", "Time", "Price", "Action", "æ³¨ç›®åº¦", "Score", "Sign", "MultiSign", "ä»•æ‰‹è­¦æˆ’"]:
            if col not in df.columns:
                df[col] = pd.Series([""] * len(df), dtype="object")
            else:
                df[col] = df[col].astype("object")

        df["Time"] = df["Time"].astype(str)

        total = len(df)
        start_time_loop = time.time()

        skipped_count = 0  # â† è¿½åŠ 

        for i, row in df.iterrows():
            val = str(row["Time"]).strip().lower()
            if val != "" and val != "nan":
                skipped_count += 1
                continue                

            if i % 100 == 0:
                elapsed = time.time() - start_time_loop
                avg_time = elapsed / (i + 1) if i > 0 else 0
                remaining = avg_time * (total - i)
                print(f"   ğŸ”„ {i+1}ä»¶ç›®ã‚’å‡¦ç†ä¸­... â±ï¸ çµŒé: {elapsed:.1f}sï½œæ®‹ã‚Šäºˆæ¸¬: {remaining:.1f}s")

            raw_symbol = None
            try:
                raw_symbol = str(row["Symbol"]).strip().replace("$", "").replace(".T", "")
                if "." in raw_symbol:
                    raw_symbol = raw_symbol.split(".")[0]
                code = raw_symbol.zfill(4)
                ticker = f"{code}.T"

                stock = yf.Ticker(ticker)
                hist = stock.history(period="6mo")  # â† æœŸé–“ã‚’å°‘ã—æ‹¡å¤§
                if hist.empty or len(hist) < 30:
                    print(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿ä¸è¶³: {ticker} - ãƒ’ã‚¹ãƒˆãƒªã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿{len(hist)}ä»¶")
                    continue

                df.at[i, "Price"] = round(hist["Close"].iloc[-1], 2)
                if pd.isnull(row["Name"]) or row["Name"] == "":
                    df.at[i, "Name"] = stock.info.get("shortName", "å–å¾—å¤±æ•—")

                info = stock.info
                signals, attention, comment, action, score_str = analyze_stock(hist, info)

                df.at[i, "Sign"] = "âœ… " + ", ".join(signals) if signals else ""
                df.at[i, "MultiSign"] = comment
                df.at[i, "Action"] = action
                df.at[i, "æ³¨ç›®åº¦"] = attention if attention else "ãƒ¼"
                df.at[i, "Score"] = score_str
                df.at[i, "Time"] = timestamp_str

                # ä»•æ‰‹è­¦æˆ’ãƒ•ãƒ©ã‚°ï¼ˆæ–‡å­—è¡¨ç¤ºï¼‰
                risk_flags = []
                if any("ä»•æ‰‹" in s for s in signals):
                    risk_flags.append("ä»•æ‰‹å±¥æ­´")
                if any("è¶…å°å‹æ ª" in s for s in signals):
                    risk_flags.append("è¶…å°å‹æ ª")
                if any("ä¿¡ç”¨å€ç‡é«˜" in s for s in signals):
                    risk_flags.append("ä¿¡ç”¨éç†±")
                # Sé«˜åˆ¤å®šï¼ˆå‰æ—¥æ¯”+30%ä»¥ä¸Šã®é«˜å€¤ï¼‰
                try:
                    prev_close = hist["Close"].iloc[-2]
                    today_high = hist["High"].iloc[-1]
                    if today_high >= prev_close * 1.3:
                        risk_flags.append("Sé«˜æ€¥é¨°ğŸŸ¢")  # â† è‰²ä»˜ãä»£æ›¿è¡¨ç¾
                except Exception as e:
                    print(f"âš ï¸ Sé«˜åˆ¤å®šå¤±æ•—: {ticker} - {e}")

                df.at[i, "ä»•æ‰‹è­¦æˆ’"] = "\n".join(risk_flags) if risk_flags else ""

            except Exception as e:
                print(f"âš ï¸ ã‚¨ãƒ©ãƒ¼: {raw_symbol or 'ä¸æ˜'} - {e}")

        # ğŸ”š å„ã‚·ãƒ¼ãƒˆå‡¦ç†å¾Œã«ã‚¹ã‚­ãƒƒãƒ—ä»¶æ•°ã‚’è¡¨ç¤º
        print(f"ğŸŸ¡ ã‚¹ã‚­ãƒƒãƒ—ã•ã‚ŒãŸå‡¦ç†æ¸ˆã¿è¡Œ: {skipped_count}ä»¶")

        # ğŸ”„ å„è¡Œå‡¦ç†ãŒçµ‚ã‚ã£ãŸå¾Œã«1å›ã ã‘è¿½åŠ 
        df = normalize_symbol_column(df)
        df = reorder_columns(df, ["Symbol", "Name", "Time", "Price", "Action", "Score", "æ³¨ç›®åº¦", "ä»•æ‰‹è­¦æˆ’", "Sign", "MultiSign"])
        df = clean_dataframe_columns(df, ["Symbol", "Name", "Time", "Price", "Action", "Score", "æ³¨ç›®åº¦", "ä»•æ‰‹è­¦æˆ’", "Sign", "MultiSign"])
        all_dfs.append(df.copy())

        output_df = df.drop(columns=["Sign"])
        df.drop(columns=[col for col in ["TrendScore", "TrendScore_raw"] if col in df.columns], inplace=True)

        worksheet.clear()
        #Signåˆ—ã‚’è¡¨ç¤ºã™ã‚‹å ´åˆã¯ã€[output_]ã‚’å‰Šé™¤
        set_with_dataframe(worksheet, output_df.fillna("").infer_objects(copy=False))

        new_sheet_title = f"Watchlist_{sheet_name}_{today_str}"
        try:
            delete_existing_file_by_name(drive_service, folder_id_backup, new_sheet_title)
            new_spreadsheet = gc.create(new_sheet_title, folder_id_backup)
            set_with_dataframe(new_spreadsheet.sheet1, output_df.fillna("").infer_objects(copy=False))
            print(f"âœ… å®Œäº†: {new_sheet_title}")
        except Exception as e:
            print(f"âŒ ä½œæˆå¤±æ•—: {new_sheet_title} - {e}")

        elapsed_sheet = time.time() - start_time_sheet
        sheet_process_logs.append({
            "sheet_name": sheet_name,
            "count": len(df),
            "elapsed": elapsed_sheet,
            "failures": [row["Symbol"] for i, row in df.iterrows() if "ã‚¨ãƒ©ãƒ¼" in str(row["MultiSign"]) or "å–å¾—å¤±æ•—" in str(row["Name"])]
        })

    # âœ… æ¡ä»¶ä¸€è‡´ã®çµ±åˆå‡ºåŠ›
    try:
        combined_df = pd.concat(all_dfs, ignore_index=True)
        combined_df["SignalCount"] = combined_df["Sign"].apply(
            lambda x: x.count(",") + 1 if isinstance(x, str) and x.startswith("âœ…") else 0
        )

        condition = (
            ((combined_df["Action"] == "è²·ã„ï¼ˆã‚¨ãƒ³ãƒˆãƒªãƒ¼æ¤œè¨ï¼‰") | (combined_df["æ³¨ç›®åº¦"] == "â˜…â˜…â˜…")) &
            (combined_df["SignalCount"] >= 3) &
            (combined_df["Score"].astype(str).str.extract(r"([\d\.]+)").astype(float)[0] >= 5.0)
        )
        # å£²ã‚ŠéŠ˜æŸ„ã®æŠ½å‡ºä¾‹
        sell_condition = (
            (combined_df["Action"] == "å£²ã‚Šï¼ˆåˆ©ç¢º/è­¦æˆ’ï¼‰") &
            (combined_df["Score"].astype(str).str.extract(r"([-]?\d+\.?\d*)").astype(float)[0] <= -2.0)
        )
        sell_df = combined_df[sell_condition].copy()

        filtered_df = combined_df[condition].copy()
        filtered_df.drop(columns=["SignalCount"], inplace=True)

        filtered_df["SortKey"] = filtered_df["Symbol"].apply(symbol_to_num)
        filtered_df.sort_values("SortKey", inplace=True)
        filtered_df.drop(columns=["SortKey"], inplace=True)

        summary_title = f"watchlist_signal_{today_str}"
        delete_existing_file_by_name(drive_service, folder_id_main, summary_title)
        summary_sheet_id = create_spreadsheet_in_folder(summary_title, folder_id_main, creds)
        summary_sheet = gc.open_by_key(summary_sheet_id)

        filtered_output_df = filtered_df.drop(columns=["Sign"])
        set_with_dataframe(summary_sheet.sheet1, filtered_output_df.fillna("").infer_objects(copy=False))

        sheet_url = f"https://docs.google.com/spreadsheets/d/{summary_sheet_id}/edit"
        print(f"ğŸ“¤ æ¡ä»¶ä¸€è‡´ã‚·ã‚°ãƒŠãƒ«å‡ºåŠ›å®Œäº†: {summary_title}")
        print(f"ğŸ”— å‡ºåŠ›å…ˆURL: {sheet_url}")

    except Exception as e:
        print(f"âš ï¸ æ¡ä»¶ä¸€è‡´ã®å‡ºåŠ›å¤±æ•—: {e}")

    print("\nğŸ”¬ ãƒ‡ãƒãƒƒã‚°æƒ…å ±:")
    print(f"å…¨ã‚·ãƒ¼ãƒˆæ•°: {len(sheet_list)}")
    print(f"å‡¦ç†æ¸ˆã¿ã‚·ãƒ¼ãƒˆæ•°: {len(all_dfs)}")
    print(f"å…¨ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(combined_df)}")
    print(f"ãƒ•ã‚£ãƒ«ã‚¿å¾Œä»¶æ•°: {len(filtered_df)}")
    print(f"ğŸ“ ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€IDï¼ˆmainï¼‰: {folder_id_main}")
    print("ğŸ‰ ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ")


# ================================
# âœ… ãƒ¡ã‚¤ãƒ³å‡¦ç†é–¢æ•°
# ================================

def main():
    # ã‚ãªãŸã®Driveä¸Šã®ç›®çš„ãƒ•ã‚©ãƒ«ãƒ€IDã«å·®ã—æ›¿ãˆã¦ãã ã•ã„
    FOLDER_ID_MAIN = "1dFuJfNLSJ7tw43Ac9RKAqJ0yW49cLsIe"       # ColabNotebooks/éŠ˜æŸ„åˆ†æ/Watchlist
    FOLDER_ID_BACKUP = "1hN6fzMeT1ZB7I0jVRc9L_8HKIqI0Gi_W"     # ColabNotebooks/éŠ˜æŸ„åˆ†æ/Watchlist/watchlist

    spreadsheet_name = "watchlist"
    mount_drive()

    start_time = time.time()
    print("ğŸš€ å‡¦ç†é–‹å§‹")

    global sheet_process_logs
    sheet_process_logs = []  # â† ãƒ­ã‚°åˆæœŸåŒ–

    try:
        _, gc, creds = authenticate_services()

        # Google Drive APIã®ã‚µãƒ¼ãƒ“ã‚¹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        drive_service = build("drive", "v3", credentials=creds)

        # ğŸ“Œ ãƒ•ã‚©ãƒ«ãƒ€IDã‚’ãã®ã¾ã¾æ¸¡ã™ï¼ˆget_drive_folder_id_by_path ã‚’ä½¿ã‚ãªã„ï¼‰
        process_all_sheets(spreadsheet_name, gc, drive_service, FOLDER_ID_MAIN, FOLDER_ID_BACKUP, creds)

    except Exception as e:
        print(f"âŒ é‡å¤§ã‚¨ãƒ©ãƒ¼: {e}")
        return

    elapsed = time.time() - start_time
    print("âœ… å…¨å‡¦ç†å®Œäº†ï¼")
    print(f"â±ï¸ æ‰€è¦æ™‚é–“: {elapsed:.1f} ç§’")

    # å„ã‚·ãƒ¼ãƒˆå˜ä½ã®ãƒ­ã‚°å‡ºåŠ›ï¼ˆçœç•¥ã›ãšå‡ºã™ï¼‰
    if sheet_process_logs:
        print("\nğŸ“Š ã‚·ãƒ¼ãƒˆã”ã¨ã®å‡¦ç†æ¦‚è¦:")
        for log in sheet_process_logs:
            print(f"\nğŸ“„ ã‚·ãƒ¼ãƒˆ: {log['sheet_name']}")
            print(f" - ä»¶æ•°: {log['count']} è¡Œ")
            print(f" - å‡¦ç†æ™‚é–“: {log['elapsed']:.1f} ç§’")
            if log['failures']:
                print(f" - âš ï¸ å¤±æ•—éŠ˜æŸ„: {len(log['failures'])} ä»¶")
                from collections import defaultdict
                grouped = defaultdict(list)
                for code in log['failures']:
                    try:
                        prefix = int(code) // 1000 * 1000
                        grouped[f"{prefix}ç•ªå°"].append(code)
                    except:
                        grouped["ä¸æ˜"].append(code)
                for group in sorted(grouped.keys()):
                    print(f"   ãƒ»{group}: [{', '.join(grouped[group])}]")
            else:
                print(" - ğŸ‰ å…¨éŠ˜æŸ„æ­£å¸¸ã«å–å¾—")
