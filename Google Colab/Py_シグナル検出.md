
#è¿½åŠ å†…å®¹
##ä»•æ‰‹æ ªã¨æ€ã‚ã‚Œã‚‹éŠ˜æŸ„ã®è­¦æˆ’æ©Ÿèƒ½#
##é«˜å€¤çªç ´åˆ¤å®š	ã€Œç›´è¿‘60æ—¥é«˜å€¤ã€ï¼‹ã€ŒæœŸé–“å†…ã®é«˜å€¤ã‚’æœ¬å½“ã«è¶…ãˆãŸã¨ãã®ã¿ã€åˆ¤å®š
##æ€¥é¨°å±¥æ­´æ¤œå‡º	ç›´è¿‘60æ—¥ã§+40%ä»¥ä¸Šã®å¤‰å‹•ã®ã¿è­¦å‘Š
##å°å‹æ ªåˆ¤å®š	æ™‚ä¾¡ç·é¡100å„„å††æœªæº€ã®ã¿ã«é™å®šã—ã€éå‰°æ¤œå‡ºã‚’é˜²æ­¢
##ã‚¹ã‚³ã‚¢åŠ ç‚¹ã‚·ã‚°ãƒŠãƒ«ã®è¤‡æ•°è¿½åŠ 
##é€±åˆ¥è©•ä¾¡åˆ†æã‚³ãƒ¼ãƒ‰ã®è¿½åŠ 

# ================================
# Sec1ï½œã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
# ================================

!pip install --upgrade gspread gspread_dataframe google-auth yfinance --quiet
!pip install ta --quiet

import os
import pandas as pd
import yfinance as yf
import gspread
from google.colab import auth
from gspread_dataframe import get_as_dataframe, set_with_dataframe
from googleapiclient.discovery import build
from googleapiclient.http import MediaInMemoryUpload
from google.auth.transport.requests import Request
from datetime import datetime, timezone, timedelta
import google.auth
from google.colab import drive
import time
from collections import defaultdict

pd.set_option('future.no_silent_downcasting', True)

# ================================
# Sec2ï½œå®šç¾©ãƒ»é–¢æ•°
# ================================

import numpy as np

SAVE_PATH = "drive/MyDrive/ColabNotebooks/éŠ˜æŸ„åˆ†æ/signal"  # ğŸ”§ ä¿å­˜å…ˆã®ãƒ‘ã‚¹ã‚’æ­£ã—ãè¨­å®š
BACKUP_SUBFOLDER = "backup"  # ğŸ”§ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã¯ã“ã®ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€ã«

# GoogleDriveé–¢æ•°
def mount_drive():
    drive.mount('/content/drive')

def authenticate_services():
    auth.authenticate_user()
    creds, _ = google.auth.default()
    return creds, gspread.authorize(creds), creds

def get_drive_folder_id_by_path(path, creds):
    service = build('drive', 'v3', credentials=creds)
    parts = path.strip("/").split("/")
    parent_id = 'root'
    for part in parts:
        query = f"name='{part}' and mimeType='application/vnd.google-apps.folder' and '{parent_id}' in parents and trashed=false"
        results = service.files().list(q=query, spaces='drive', fields='files(id, name)').execute()
        files = results.get('files', [])
        if files:
            parent_id = files[0]['id']
        else:
            file_metadata = {
                'name': part,
                'mimeType': 'application/vnd.google-apps.folder',
                'parents': [parent_id]
            }
            file = service.files().create(body=file_metadata, fields='id').execute()
            parent_id = file['id']
    return parent_id, service

def delete_existing_file_by_name(drive_service, folder_id, file_name):
    query = f"'{folder_id}' in parents and name = '{file_name}' and mimeType = 'application/vnd.google-apps.spreadsheet'"
    results = drive_service.files().list(q=query, fields="files(id, name)").execute()
    files = results.get('files', [])
    for file in files:
        drive_service.files().delete(fileId=file['id']).execute()

def create_spreadsheet_in_folder(title, folder_id, creds):
    drive_service = build("drive", "v3", credentials=creds)
    file_metadata = {
        "name": title,
        "mimeType": "application/vnd.google-apps.spreadsheet",
        "parents": [folder_id]
    }
    file = drive_service.files().create(body=file_metadata, fields="id").execute()
    return file["id"]

# æ—¥ä»˜ã®å–å¾—
def get_today_str():
    JST = timezone(timedelta(hours=9))
    now = datetime.now(JST)
    return now.strftime("%Y-%m-%d"), now.strftime("%Y-%m-%d %H:%M:%S")

# ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®Colum
def reorder_columns(df, preferred_order):
    existing_cols = [col for col in preferred_order if col in df.columns]
    other_cols = [col for col in df.columns if col not in existing_cols]
    return df[existing_cols + other_cols]

def symbol_to_num(symbol):
    # .Tã‚’é™¤å»ã—ã€æ•°å€¤éƒ¨åˆ†ã®ã¿æŠ½å‡º
    s = str(symbol).replace('.T', '')
    try:
        return int(float(s))
    except ValueError:
        return np.nan  # æ•°å€¤åŒ–ã§ããªã„å ´åˆã¯NaN

def clean_dataframe_columns(df, expected_cols):
    """
    DataFrameã‹ã‚‰é‡è¤‡åˆ—ï¼ˆä¾‹: 'MultiSign.1'ï¼‰ã‚’å–ã‚Šé™¤ãã€åˆ—é †ã‚’expected_colsã«æ•´ãˆã‚‹ã€‚
    """
    # 1. æœŸå¾…ã•ã‚Œã‚‹åˆ—åã«å¯¾ã—ã¦ã€.1 ã‚„ .2 ãŒã¤ã„ãŸé‡è¤‡åˆ—ãŒã‚ã‚Œã°å‰Šé™¤
    deduped_cols = {}
    for col in df.columns:
        base_col = col.split('.')[0]
        if base_col not in deduped_cols:
            deduped_cols[base_col] = col  # æœ€åˆã«å‡ºã¦ããŸæ­£è¦åã‚’æ¡ç”¨

    df = df[list(deduped_cols.values())]

    # 2. åˆ—é †ã‚’expected_colsã®é †ã«æ•´ãˆã‚‹ï¼ˆå­˜åœ¨ã™ã‚‹åˆ—ã®ã¿ï¼‰
    df = df[[col for col in expected_cols if col in df.columns]]

    return df

def normalize_symbol_column(df):
    """
    Symbolåˆ—ã‚’ "XXXX.T" å½¢å¼ã«çµ±ä¸€ã™ã‚‹
    - floatå½¢å¼ã® .0 é™¤å»
    - .T ã®é‡è¤‡é™¤å»
    - .0.T ã®èª¤å¤‰æ›é™¤å»
    """
    def clean_symbol(val):
        try:
            val = str(val).strip().replace('$', '')
            val = val.replace('.0.T', '')  # â† è¿½åŠ ãƒã‚¤ãƒ³ãƒˆ
            if val.endswith('.0'):
                val = val[:-2]
            val = val.replace('.T', '')  # é‡è¤‡é™¤å»
            return val + '.T'
        except:
            return ""

    if "Symbol" in df.columns:
        df["Symbol"] = df["Symbol"].apply(clean_symbol)
    return df

import yfinance as yf

# æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã¨ä¼šç¤¾åã‚’å–å¾—ã™ã‚‹é–¢æ•°
def get_stock_data(symbol):
    try:
        ticker = yf.Ticker(symbol)
        info = ticker.info
        name = info.get("shortName", symbol)

        df = yf.download(symbol, period="18mo", interval="1d", auto_adjust=False)

        if df.empty:
            raise ValueError("å–å¾—çµæœãŒç©ºã§ã™")

        if isinstance(df.columns, pd.MultiIndex):
            df.columns = df.columns.get_level_values(0)

        df = df.dropna(subset=["Open", "High", "Low", "Close", "Volume"]).astype(float)
        df.index.name = "Date"
        return df, name

    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {symbol} - {e}")
        return pd.DataFrame(), symbol

# ================================
# å®Œå…¨çµ±åˆã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆå®šç¾© + å‡ºåŠ›å¯¾å¿œï¼‰
# ================================

import pandas as pd
import numpy as np
from ta.trend import ADXIndicator, MACD
from ta.momentum import RSIIndicator

# ã‚¹ã‚³ã‚¢è¾æ›¸ï¼ˆæ³¨ç›®åº¦ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆãƒ»ç‚¹æ•°ï¼‰
LEVELS = {
    "â˜…â˜…â˜…": {
        "keywords": [
            {"text": "ä¸Šå ´æ¥é«˜å€¤çªç ´", "comment": "ä¸Šå ´æ¥é«˜å€¤ã‚’æ›´æ–°ã—æ³¨ç›®åº¦MAX"},
            {"text": "ç›´è¿‘é«˜å€¤çªç ´", "comment": "ç›´è¿‘é«˜å€¤ã‚’çªç ´ã—ã€ãƒˆãƒ¬ãƒ³ãƒ‰ç¶™ç¶šã®å¯èƒ½æ€§"},
            {"text": "å‡ºæ¥é«˜æ€¥å¢—", "comment": "å‡ºæ¥é«˜ãŒæ€¥å¢—ã—ã€å¤§å£ã®è²·ã„ãŒå…¥ã£ã¦ã„ã‚‹å¯èƒ½æ€§"},
            {"text": "æœŸé–“å†…é«˜å€¤æ›´æ–°ï¼ˆè¦æ³¨ç›®ï¼‰", "comment": "å–å¾—å¯èƒ½æœŸé–“å†…ã®æœ€é«˜å€¤ã‚’æ›´æ–°ã—ã¦ãŠã‚Šã€æ³¨ç›®åº¦ã¯é«˜ã„ã§ã™ã€‚"},
            {"text": "é€±è¶³é«˜å€¤ãƒ–ãƒ¬ã‚¤ã‚¯", "comment": "é€±è¶³ã§ã‚‚é«˜å€¤ã‚’æ›´æ–°ã—ã¦ãŠã‚Šã€ä¸­é•·æœŸçš„ãªå¼·ã•ãŒä¼ºãˆã¾ã™ã€‚"},
            {"text": "MACDé€±è¶³é™½è»¢", "comment": "é€±è¶³ãƒ™ãƒ¼ã‚¹ã®MACDãŒé™½è»¢ã—ã€ä¸­é•·æœŸã§è²·ã„ã®å‹¢ã„ãŒå‡ºã¦ã„ã¾ã™ã€‚"}
        ],
        "score": 3
    },
    "â˜…â˜…": {
        "keywords": [
            {"text": "MACDé™½è»¢", "comment": "MACDãŒé™½è»¢ã—ã€è²·ã„ã®å‹¢ã„ãŒå‡ºå§‹ã‚ã¦ã„ã¾ã™ã€‚"},
            {"text": "çŸ­æœŸã‚´ãƒ¼ãƒ«ãƒ‡ãƒ³ã‚¯ãƒ­ã‚¹", "comment": "çŸ­æœŸã®ç§»å‹•å¹³å‡ç·šãŒã‚´ãƒ¼ãƒ«ãƒ‡ãƒ³ã‚¯ãƒ­ã‚¹ã‚’å½¢æˆã—ã¦ã„ã¾ã™ã€‚"},
            {"text": "ä¸­æœŸã‚´ãƒ¼ãƒ«ãƒ‡ãƒ³ã‚¯ãƒ­ã‚¹", "comment": "ä¸­æœŸã®ç§»å‹•å¹³å‡ç·šã‚‚ã‚´ãƒ¼ãƒ«ãƒ‡ãƒ³ã‚¯ãƒ­ã‚¹ã—ã€ãƒˆãƒ¬ãƒ³ãƒ‰ã®æŒç¶šæ€§ãŒé«˜ã¾ã£ã¦ã„ã¾ã™ã€‚"},
            {"text": "RSIåç™º", "comment": "RSIãŒåç™ºã—ã€åè»¢ä¸Šæ˜‡ã®å…†ã—ãŒã‚ã‚Šã¾ã™ã€‚"},
            {"text": "å‡ºæ¥é«˜â†‘", "comment": "å‡ºæ¥é«˜ãŒå¢—ãˆã¦ãŠã‚Šæ³¨ç›®ãŒé›†ã¾ã£ã¦ã„ã¾ã™ã€‚"},
            {"text": "ä¸‰é€£ç¶šé™½ç·š", "comment": "3æ—¥é€£ç¶šã®é™½ç·šãŒç¶šã„ã¦ãŠã‚Šã€å¼·ã„ä¸Šæ˜‡åœ§åŠ›ãŒç¢ºèªã§ãã¾ã™ã€‚"},
            {"text": "é™½ã®ä¸¸åŠä¸»", "comment": "é™½ã®ä¸¸åŠä¸»ãŒå‡ºç¾ã—ã€è²·ã„åœ§åŠ›ã®å¼·ã•ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚"},
            {"text": "ä¸‹å€¤åˆ‡ã‚Šä¸Šã’ç¶™ç¶š", "comment": "å®‰å€¤ãŒåˆ‡ã‚Šä¸ŠãŒã‚‹å±•é–‹ãŒç¶šã„ã¦ãŠã‚Šã€ä¸‹æ”¯ãˆã®å¼·ã•ãŒè¦‹ã‚‰ã‚Œã¾ã™ã€‚"}
        ],
        "score": 2
    },
    "â˜…": {
        "keywords": [
            {"text": "çª“é–‹ã‘ä¸Šæ˜‡", "comment": "çª“ã‚’é–‹ã‘ã¦ä¸Šæ˜‡ã—å‹¢ã„ã‚ã‚Š"},
            {"text": "ä¸‹é«­é™½ç·š", "comment": "ä¸‹é«­é™½ç·šãŒå‡ºç¾ã—ã€æŠ¼ã—ç›®è²·ã„ã®å¯èƒ½æ€§ã‚ã‚Š"},
            {"text": "æŠ¼ã—ç›®é™½ç·š", "comment": "èª¿æ•´å¾Œã®æŠ¼ã—ç›®ã§é™½ç·šãŒå‡ºã¦ãŠã‚Šã€åç™ºã®å…†å€™ãŒè¦‹ã‚‰ã‚Œã¾ã™ã€‚"},
            {"text": "ä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³åç™º", "comment": "ä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ä»˜è¿‘ã§åç™ºã—ã¦ãŠã‚Šã€ä¸‹å€¤æ”¯æŒãŒç¢ºèªã§ãã¾ã™ã€‚"}
        ],
        "score": 1
    }
}

def adjust_attention_by_warning(attention, comment):
    """
    è­¦æˆ’ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚Œã°ã€æ³¨ç›®åº¦ã‚’â˜…â†’â˜†ã«1æ®µéšå¤‰æ›ã™ã‚‹
    """
    warning_keywords = ["ä»•æ‰‹", "éç†±", "æ³¨æ„", "è­¦æˆ’", "ä¿¡ç”¨å€ç‡é«˜", "ä¸Šé«­é™°ç·š", "è¶…å°å‹æ ª"]
    if any(k in comment for k in warning_keywords):
        if attention == "â˜…â˜…â˜…":
            return "â˜…â˜…â˜†"
        elif attention == "â˜…â˜…":
            return "â˜…â˜†"
        elif attention == "â˜…":
            return "â˜†"
    return attention

# ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆï¼†è©•ä¾¡é–¢æ•°

import random

def get_summary_by_attention(attention):
    summaries = {
        "â˜…": [
            "åè»¢ã®å…†ã—ã¯ã‚ã‚‹ã‚‚ã®ã®ã€ã¾ã æ˜ç¢ºãªãƒˆãƒ¬ãƒ³ãƒ‰ã¯å½¢æˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚",
            "ã‚„ã‚„è²·ã„åœ§åŠ›ãŒè¦‹ãˆå§‹ã‚ã¾ã—ãŸãŒã€æ§˜å­è¦‹ãŒå¦¥å½“ãªå ´é¢ã§ã™ã€‚",
            "åˆå‹•ã®å¯èƒ½æ€§ã‚‚ã‚ã‚‹ãŸã‚ã€ä»Šå¾Œã®å±•é–‹ã«æ³¨è¦–ã—ãŸã„å±€é¢ã§ã™ã€‚"
        ],
        "â˜…â˜…": [
            "è¤‡æ•°ã®ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ãŒå¥½è»¢ã—ã¦ãŠã‚Šã€æ‰“è¨ºè²·ã„ã‚’æ¤œè¨ã™ã‚‹ä½™åœ°ãŒã‚ã‚Šã¾ã™ã€‚",
            "ä¸€å®šã®ãƒˆãƒ¬ãƒ³ãƒ‰ã‚·ã‚°ãƒŠãƒ«ãŒæƒã„ã¯ã˜ã‚ã¦ãŠã‚Šã€ä»Šå¾Œã®ä¸Šæ˜‡ä½™åœ°ã«æ³¨ç›®ã§ã™ã€‚",
            "éç†±æ„Ÿã«ã¯æ³¨æ„ãŒå¿…è¦ã§ã™ãŒã€åè»¢å±€é¢ã«å…¥ã£ãŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
        ],
        "â˜…â˜…â˜…": [
            "ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«çš„ã«ã¯è²·ã„ã‚µã‚¤ãƒ³ãŒæ˜ç¢ºã«æƒã£ã¦ãŠã‚Šã€ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã®å¥½æ©Ÿã¨åˆ¤æ–­ã•ã‚Œã¾ã™ã€‚",
            "å¼·ã„ãƒˆãƒ¬ãƒ³ãƒ‰ãŒç™ºç”Ÿã—ã¦ãŠã‚Šã€å¤§ç›¸å ´å…¥ã‚Šã®å¯èƒ½æ€§ã™ã‚‰ç¤ºå”†ã•ã‚Œã¦ã„ã¾ã™ã€‚",
            "ã‚·ã‚°ãƒŠãƒ«ãŒè¤‡æ•°ä¸€è‡´ã—ã€è²·ã„å‹¢åŠ›ãŒå„ªå‹¢ã€‚ä»•è¾¼ã¿æ™‚ã¨ã—ã¦æœ‰æœ›ã§ã™ã€‚"
        ],
        "ãƒ¼": [
            "ä»Šã®ã¨ã“ã‚æ˜ç¢ºãªãƒˆãƒ¬ãƒ³ãƒ‰ã¯ç¢ºèªã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
        ]
    }
    return "â¡ ç·è©•ï¼š" + random.choice(summaries.get(attention, summaries["ãƒ¼"]))

def analyze_signals(signals, adx_value=None):
    total_score = 0
    comments = []

    # é‡è¤‡ã‚·ã‚°ãƒŠãƒ«ã®é™¤å»
    if "å‡ºæ¥é«˜â†‘" in signals and "å‡ºæ¥é«˜æ€¥å¢—" in signals:
        signals = [s for s in signals if s != "å‡ºæ¥é«˜â†‘"]

    for level in LEVELS.values():
        for item in level["keywords"]:
            if item["text"] in signals:
                total_score += level["score"]
                comments.append(f"âœ… {item['comment']}")

    if adx_value:
        if adx_value >= 40:
            total_score += 1
            comments.append("â˜…å¼·ã„ãƒˆãƒ¬ãƒ³ãƒ‰ãŒç¢ºèªã•ã‚Œã¦ãŠã‚Šã€é †å¼µã‚Šã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã¨ã—ã¦æœ‰åŠ¹ã§ã™ã€‚")
        elif adx_value >= 25:
            comments.append("â˜†ç¾åœ¨ã¯ä¸­ç¨‹åº¦ã®ãƒˆãƒ¬ãƒ³ãƒ‰ãŒç™ºç”Ÿã—ã¦ãŠã‚Šã€æ–¹å‘æ„ŸãŒå‡ºå§‹ã‚ã¦ã„ã¾ã™ã€‚")

    # period_high åˆ¤å®šã« High ã‚’å«ã‚ã‚‹å‡¦ç†ã‚’åˆ¥é€” analyze_stock å´ã§è¡Œã†æƒ³å®šï¼ˆã“ã®é–¢æ•°å†…ã«ã¯ä¸è¦ï¼‰

    if total_score >= 6:
        attention = "â˜…â˜…â˜…"
    elif total_score >= 3:
        attention = "â˜…â˜…"
    elif total_score >= 1:
        attention = "â˜…"
    else:
        attention = "ãƒ¼"

    summary = get_summary_by_attention(attention)
    full_comment = "".join(comments + [summary])
    #score_str = f"{total_score:.1f} / 10.0"
    score_str = f"{total_score:.1f} / {max(total_score, 10):.1f}"  # â†å¤‰æ›´ã“ã“

    return attention, full_comment, total_score, score_str

# ã‚³ãƒ¡ãƒ³ãƒˆå†…å®¹ã‹ã‚‰ã‚¢ã‚¯ã‚·ãƒ§ãƒ³åˆ¤å®šï¼ˆæ³¨ç›®åº¦ãƒ™ãƒ¼ã‚¹ï¼‹è­¦å‘Šè£œæ­£ã‚ã‚Šï¼‰
def judge_action_by_comment(comment, attention=None):
    if attention in ["â˜…â˜…â˜…", "â˜…â˜…â˜†"]:
        return "è²·ã„ï¼ˆã‚¨ãƒ³ãƒˆãƒªãƒ¼æ¤œè¨ï¼‰"
    elif attention in ["â˜…â˜…", "â˜…â˜†"]:
        return "è²·ã„ï¼ˆã‚¨ãƒ³ãƒˆãƒªãƒ¼æ¤œè¨ï¼‰"
    elif attention in ["â˜…", "â˜†"]:
        return "ä¸­ç«‹ï¼ˆæ§˜å­è¦‹ï¼‰"
    else:
        return "ä¸­ç«‹ï¼ˆæ§˜å­è¦‹ï¼‰"

# ãƒ­ãƒ¼ã‚½ã‚¯è¶³ãƒ‘ã‚¿ãƒ¼ãƒ³ç°¡æ˜“æ¤œå‡ºï¼ˆä»»æ„è¿½åŠ å¯èƒ½ï¼‰
def detect_candlestick_patterns(df):
    patterns = []
    open_, close, high, low = df.iloc[-1][["Open", "Close", "High", "Low"]]
    body = abs(close - open_)
    lower_shadow = min(open_, close) - low
    upper_shadow = high - max(open_, close)
    if close > open_ and lower_shadow > body * 2:
        patterns.append("ä¸‹é«­é™½ç·š")
    if close > open_ and upper_shadow < body * 0.1 and lower_shadow < body * 0.1:
        patterns.append("é™½ã®ä¸¸åŠä¸»")
    return patterns

# === é€±è¶³ã‚·ã‚°ãƒŠãƒ«æ¤œå‡º ===
def detect_weekly_signals(df):
    signals = []
    df_weekly = df.resample("W-FRI").last().dropna()
    if len(df_weekly) < 20:
        return signals

    # é€±è¶³é«˜å€¤ãƒ–ãƒ¬ã‚¤ã‚¯
    if df_weekly["Close"].iloc[-1] > df_weekly["High"].iloc[-20:-1].max():
        signals.append("é€±è¶³é«˜å€¤ãƒ–ãƒ¬ã‚¤ã‚¯")

    # MACDé€±è¶³é™½è»¢
    macd = MACD(df_weekly["Close"])
    macd_line = macd.macd()
    macd_signal = macd.macd_signal()
    if macd_line.iloc[-2] < macd_signal.iloc[-2] and macd_line.iloc[-1] > macd_signal.iloc[-1]:
        signals.append("MACDé€±è¶³é™½è»¢")

    return signals

# === ä¸­æœŸGCæ¤œå‡º ===
def detect_mid_term_golden_cross(df):
    if len(df) < 75:
        return []
    ma25 = df["Close"].rolling(window=25).mean()
    ma75 = df["Close"].rolling(window=75).mean()
    cross = (ma25.shift(1) < ma75.shift(1)) & (ma25 > ma75)
    return ["ä¸­æœŸã‚´ãƒ¼ãƒ«ãƒ‡ãƒ³ã‚¯ãƒ­ã‚¹"] if cross.iloc[-1] else []

# === æŠ¼ã—ç›®é™½ç·šæ¤œå‡º ===
def detect_pullback_bounce(df):
    if len(df) < 3:
        return []
    high_break = df["Close"].iloc[-3] > df["High"].iloc[-20:-4].max()
    pullback = df["Close"].iloc[-2] < df["Close"].iloc[-3]
    rebound = df["Close"].iloc[-1] > df["Open"].iloc[-1]
    if high_break and pullback and rebound:
        return ["æŠ¼ã—ç›®é™½ç·š"]
    return []

# === ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³åç™ºæ¤œå‡º ===
def detect_trendline_bounce(df):
    if len(df) < 10:
        return []
    lows = df["Low"].iloc[-10:]
    x = np.arange(len(lows))
    coef = np.polyfit(x, lows, 1)
    trendline = coef[0] * x + coef[1]
    if lows.iloc[-1] > trendline[-1]:
        return ["ä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³åç™º"]
    return []

# === ä¸‹å€¤åˆ‡ã‚Šä¸Šã’æ¤œå‡º ===
def detect_higher_lows(df):
    if len(df) < 3:
        return []
    lows = df["Low"].iloc[-3:]
    if lows.iloc[0] < lows.iloc[1] < lows.iloc[2]:
        return ["ä¸‹å€¤åˆ‡ã‚Šä¸Šã’ç¶™ç¶š"]
    return []

# âœ… analyze_stock() é–¢æ•°ï¼ˆä»•æ‰‹æ ªãƒªã‚¹ã‚¯ï¼‹é«˜å€¤çªç ´ä¿®æ­£æ¸ˆã¿ï¼‰
def analyze_stock(df, info=None):
    if len(df) < 30:
        return [], "ãƒ¼", "", "ä¸­ç«‹ï¼ˆæ§˜å­è¦‹ï¼‰", 0, ""

    df = df.copy()
    close = df["Close"]
    high = df["High"]
    low = df["Low"]
    open_ = df["Open"]
    volume = df["Volume"]

    # ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™
    df["RSI"] = RSIIndicator(close, window=14).rsi()
    macd = MACD(close)
    df["MACD"] = macd.macd()
    df["MACD_signal"] = macd.macd_signal()
    adx = ADXIndicator(high, low, close, window=14)
    df["ADX"] = adx.adx()

    signals = []

    # ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ã‚·ã‚°ãƒŠãƒ«
    if df["MACD"].iloc[-2] < df["MACD_signal"].iloc[-2] and df["MACD"].iloc[-1] > df["MACD_signal"].iloc[-1]:
        signals.append("MACDé™½è»¢")
    if df["RSI"].iloc[-2] < 30 and df["RSI"].iloc[-1] > 30:
        signals.append("RSIåç™º")
    if volume.iloc[-1] > volume.rolling(5).mean().iloc[-2] * 1.5:
        signals.append("å‡ºæ¥é«˜â†‘")
    if volume.iloc[-1] > volume.rolling(5).mean().iloc[-2] * 2.0:
        signals.append("å‡ºæ¥é«˜æ€¥å¢—")

    # ç›´è¿‘60å–¶æ¥­æ—¥é«˜å€¤ãƒ–ãƒ¬ã‚¤ã‚¯
    recent_high = df["High"].iloc[-60:-1].max()
    if close.iloc[-1] > recent_high:
        signals.append("ç›´è¿‘é«˜å€¤çªç ´")

    # Highã‚’å«ã‚€æœŸé–“å†…é«˜å€¤åˆ¤å®š
    period_high = df["High"].max()
    if max(close.iloc[-1], high.iloc[-1]) > period_high:
        signals.append("æœŸé–“å†…é«˜å€¤æ›´æ–°ï¼ˆè¦æ³¨ç›®ï¼‰")

    # ãƒ­ãƒ¼ã‚½ã‚¯è¶³ãƒ»å±é™ºãƒ‘ã‚¿ãƒ¼ãƒ³
    signals += detect_candlestick_patterns(df)
    signals += detect_danger_signals(df)

    # æ€¥é¨°å±¥æ­´
    spike_flag = detect_spike_history(df, threshold=0.4)
    if spike_flag:
        signals.append(spike_flag)

    # ãƒ•ã‚¡ãƒ³ãƒ€ãƒ¡ãƒ³ã‚¿ãƒ«è­¦å‘Š
    if info:
        signals += detect_risky_fundamentals(info)

    # æ–°è¦è¿½åŠ ã‚·ã‚°ãƒŠãƒ«ã®çµ±åˆ
    signals += detect_weekly_signals(df)
    signals += detect_mid_term_golden_cross(df)
    signals += detect_pullback_bounce(df)
    signals += detect_trendline_bounce(df)
    signals += detect_higher_lows(df)

    adx_last = df["ADX"].iloc[-1]
    attention, comment, score, score_str = analyze_signals(signals, adx_last)
    
    # â­ è­¦æˆ’ã‚³ãƒ¡ãƒ³ãƒˆã‚’ã‚‚ã¨ã«æ³¨ç›®åº¦ã‚’èª¿æ•´
    adjusted_attention = adjust_attention_by_warning(attention, comment)
    action = judge_action_by_comment(comment, adjusted_attention)

    return signals, adjusted_attention, comment, action, score_str

# è¿½åŠ é–¢æ•°ç¾¤

def detect_danger_signals(df):
    signals = []
    open_, close, high, low = df.iloc[-1][["Open", "Close", "High", "Low"]]
    body = abs(close - open_)
    upper_shadow = high - max(open_, close)
    if upper_shadow > body * 2 and close < open_:
        signals.append("ä¸Šé«­é™°ç·šï¼ˆè­¦æˆ’ï¼‰")
    return signals

# ä¿®æ­£ç‰ˆï¼šæ€¥é¨°å±¥æ­´åˆ¤å®šï¼ˆ40%ä»¥ä¸Š + ç›´è¿‘60æ—¥é–“ï¼‰
def detect_spike_history(df, threshold=0.4):
    recent_df = df[-60:]
    max_close = recent_df["Close"].max()
    min_close = recent_df["Close"].min()
    if (max_close - min_close) / min_close >= threshold:
        return "æ€¥é¨°å±¥æ­´ã‚ã‚Šï¼ˆä»•æ‰‹æ³¨æ„ï¼‰"
    return ""

# ä¿®æ­£ç‰ˆï¼šæ™‚ä¾¡ç·é¡100å„„æœªæº€ã‚’è­¦å‘Šï¼ˆä¿¡ç”¨å€ç‡å«ã‚€ï¼‰
def detect_risky_fundamentals(info):
    warnings = []
    if info.get("marketCap", 1e12) < 10_000_000_000:
        warnings.append("è¶…å°å‹æ ªï¼ˆæ³¨æ„ï¼‰")
    if info.get("shortRatio", 0) > 3.0:
        warnings.append("ä¿¡ç”¨å€ç‡é«˜ï¼ˆéç†±æ„Ÿï¼‰")
    return warnings

def process_all_sheets(spreadsheet_name, gc, drive_service, folder_id_main, folder_id_backup, creds):
    sheet_process_logs = []
    global SAVE_PATH
    today_str, timestamp_str = get_today_str()
    spreadsheet = gc.open(spreadsheet_name)
    sheet_list = spreadsheet.worksheets()

    all_dfs = []  # å„ã‚·ãƒ¼ãƒˆã®DFã‚’æ ¼ç´

    for worksheet in sheet_list:
        start_time_sheet = time.time()
        sheet_name = worksheet.title
        print(f"\U0001f50d å‡¦ç†ä¸­: {sheet_name}")
        df = get_as_dataframe(worksheet, evaluate_formulas=True)

        if "Symbol" not in df.columns:
            print(f"âš ï¸ {sheet_name} ã«Symbolåˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            continue

        # ä¿®æ­£ï¼šæ—¢å­˜åˆ—ã®å‹ã‚’ object ã«å¤‰æ›´
        for col in ["Name", "Time", "Price", "Action", "æ³¨ç›®åº¦", "Score", "Sign", "MultiSign"]:
            if col not in df.columns:
                df[col] = pd.Series([""] * len(df), dtype="object")
            else:
                df[col] = df[col].astype("object")

        df["Time"] = df["Time"].astype(str)

        # ğŸ” ãƒ«ãƒ¼ãƒ—å†…
        total = len(df)
        start_time_loop = time.time()  # ğŸ”„ å…¨ä½“ã®é–‹å§‹æ™‚é–“

        for i, row in df.iterrows():
            if i % 100 == 0:
                elapsed = time.time() - start_time_loop
                avg_time_per_row = elapsed / (i + 1) if i > 0 else 0
                remaining = avg_time_per_row * (total - i)
                print(f"   ğŸ”„ {i+1}ä»¶ç›®ã‚’å‡¦ç†ä¸­... â±ï¸ çµŒé: {elapsed:.1f}sï½œæ®‹ã‚Šäºˆæ¸¬: {remaining:.1f}s")

            raw_symbol = None
            try:
                raw_symbol = str(row["Symbol"]).strip().replace("$", "").replace(".T", "")
                if "." in raw_symbol:
                    raw_symbol = raw_symbol.split(".")[0]
                code = raw_symbol.zfill(4)
                ticker = f"{code}.T"

                stock = yf.Ticker(ticker)
                hist = stock.history(period="3mo")
                if hist.empty or len(hist) < 30:
                    print(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿ä¸è¶³: {ticker} - ãƒ’ã‚¹ãƒˆãƒªã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿{len(hist)}ä»¶")
                    continue

                df.at[i, "Price"] = round(hist["Close"].iloc[-1], 2)
                if pd.isnull(row["Name"]) or row["Name"] == "":
                    df.at[i, "Name"] = stock.info.get("shortName", "å–å¾—å¤±æ•—")

                # âœ… çµ±åˆç‰ˆ analyze_stock é–¢æ•°ã«å¯¾å¿œ
                info = stock.info  # â† è¿½åŠ 
                signals, attention, comment, action, score_str = analyze_stock(hist, info)  # â† å¼•æ•°ã«infoã‚’è¿½åŠ 

                df.at[i, "Sign"] = "âœ… " + ", ".join(signals) if signals else ""
                df.at[i, "MultiSign"] = comment
                df.at[i, "Action"] = action
                df.at[i, "æ³¨ç›®åº¦"] = attention if attention else "ãƒ¼"
                df.at[i, "Score"] = score_str
                df.at[i, "Time"] = timestamp_str

            except Exception as e:
                print(f"âš ï¸ ã‚¨ãƒ©ãƒ¼: {raw_symbol or 'ä¸æ˜'} - {e}")

        # Symbolåˆ—ã‚’æ­£è¦åŒ–ã™ã‚‹
        df = normalize_symbol_column(df)

        df = reorder_columns(df, ["Symbol", "Name", "Time", "Price", "Action", "Score", "æ³¨ç›®åº¦", "Sign", "MultiSign"])
        df = clean_dataframe_columns(df, ["Symbol", "Name", "Time", "Price","Action", "Score","æ³¨ç›®åº¦", "Sign", "MultiSign"])

        all_dfs.append(df.copy())

        df.drop(columns=[col for col in ["TrendScore", "TrendScore_raw"] if col in df.columns], inplace=True)
        worksheet.clear()
        set_with_dataframe(worksheet, df.fillna("").infer_objects(copy=False))

        new_sheet_title = f"Watchlist_{sheet_name}_{today_str}"
        try:
            delete_existing_file_by_name(drive_service, folder_id_backup, new_sheet_title)
            new_spreadsheet = gc.create(new_sheet_title, folder_id_backup)
            set_with_dataframe(new_spreadsheet.sheet1, df.fillna("").infer_objects(copy=False))
            print(f"âœ… å®Œäº†: {new_sheet_title}")
        except Exception as e:
            print(f"âŒ ä½œæˆå¤±æ•—: {new_sheet_title} - {e}")

        elapsed_sheet = time.time() - start_time_sheet
        sheet_process_logs.append({
            "sheet_name": sheet_name,
            "count": len(df),
            "elapsed": elapsed_sheet,
            "failures": [row["Symbol"] for i, row in df.iterrows() if "ã‚¨ãƒ©ãƒ¼" in str(row["MultiSign"]) or "å–å¾—å¤±æ•—" in str(row["Name"])]
        })

    # âœ… æ¡ä»¶ä¸€è‡´éŠ˜æŸ„ã®çµ±åˆã¨å‡ºåŠ›
    try:
        combined_df = pd.concat(all_dfs, ignore_index=True)

        # âœ… Signalæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        combined_df["SignalCount"] = combined_df["Sign"].apply(
            lambda x: x.count(",") + 1 if isinstance(x, str) and x.startswith("âœ…") else 0
        )

        # âœ… æ¡ä»¶ä¸€è‡´: Actionã¾ãŸã¯æ³¨ç›®åº¦ + SignalãŒ3å€‹ä»¥ä¸Š + ã‚¹ã‚³ã‚¢5.0ä»¥ä¸Š
        condition = (
            ((combined_df["Action"] == "è²·ã„ï¼ˆã‚¨ãƒ³ãƒˆãƒªãƒ¼æ¤œè¨ï¼‰") | (combined_df["æ³¨ç›®åº¦"] == "â˜…â˜…â˜…")) &
            (combined_df["SignalCount"] >= 3) &
            (combined_df["Score"].astype(str).str.extract(r"([\d\.]+)").astype(float)[0] >= 5.0)
        )
        filtered_df = combined_df[condition].copy()
        filtered_df.drop(columns=["SignalCount"], inplace=True)

        # âœ… ã‚½ãƒ¼ãƒˆã¨å‡ºåŠ›å‡¦ç†
        filtered_df["SortKey"] = filtered_df["Symbol"].apply(symbol_to_num)
        filtered_df.sort_values("SortKey", inplace=True)
        filtered_df.drop(columns=["SortKey"], inplace=True)

        summary_title = f"watchlist_signal_{today_str}"
        delete_existing_file_by_name(drive_service, folder_id_main, summary_title)
        summary_sheet_id = create_spreadsheet_in_folder(summary_title, folder_id_main, creds)
        summary_sheet = gc.open_by_key(summary_sheet_id)
        set_with_dataframe(summary_sheet.sheet1, filtered_df.fillna("").infer_objects(copy=False))

        sheet_url = f"https://docs.google.com/spreadsheets/d/{summary_sheet_id}/edit"
        print(f"ğŸ“¤ æ¡ä»¶ä¸€è‡´ã‚·ã‚°ãƒŠãƒ«å‡ºåŠ›å®Œäº†: {summary_title}")
        print(f"ğŸ”— å‡ºåŠ›å…ˆURL: {sheet_url}")

    except Exception as e:
        print(f"âš ï¸ æ¡ä»¶ä¸€è‡´ã®å‡ºåŠ›å¤±æ•—: {e}")

    # === ãƒ‡ãƒãƒƒã‚°å‡ºåŠ› ===
    print("\nğŸ”¬ ãƒ‡ãƒãƒƒã‚°æƒ…å ±:")
    print(f"å…¨ã‚·ãƒ¼ãƒˆæ•°: {len(sheet_list)}")
    print(f"å‡¦ç†æ¸ˆã¿ã‚·ãƒ¼ãƒˆæ•°: {len(all_dfs)}")
    print(f"å…¨ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(combined_df)}")
    print(f"ãƒ•ã‚£ãƒ«ã‚¿å¾Œä»¶æ•°: {len(filtered_df)}")
    print(f"ğŸ“ ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€IDï¼ˆmainï¼‰: {folder_id_main}")
    print("ğŸ‰ ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ")

def create_spreadsheet_in_folder(title, folder_id, creds):
    drive_service = build("drive", "v3", credentials=creds)
    file_metadata = {
        "name": title,
        "mimeType": "application/vnd.google-apps.spreadsheet",
        "parents": [folder_id]
    }
    file = drive_service.files().create(body=file_metadata, fields="id").execute()
    return file["id"]


# ================================
# âœ… ãƒ¡ã‚¤ãƒ³å‡¦ç†é–¢æ•°
# ================================

def main():
    # ã‚ãªãŸã®Driveä¸Šã®ç›®çš„ãƒ•ã‚©ãƒ«ãƒ€IDã«å·®ã—æ›¿ãˆã¦ãã ã•ã„
    FOLDER_ID_MAIN = "1dFuJfNLSJ7tw43Ac9RKAqJ0yW49cLsIe"       # ColabNotebooks/éŠ˜æŸ„åˆ†æ/Watchlist
    FOLDER_ID_BACKUP = "1hN6fzMeT1ZB7I0jVRc9L_8HKIqI0Gi_W"     # ColabNotebooks/éŠ˜æŸ„åˆ†æ/Watchlist/watchlist

    spreadsheet_name = "watchlist"
    mount_drive()

    start_time = time.time()
    print("ğŸš€ å‡¦ç†é–‹å§‹")

    global sheet_process_logs
    sheet_process_logs = []  # â† ãƒ­ã‚°åˆæœŸåŒ–

    try:
        _, gc, creds = authenticate_services()

        # Google Drive APIã®ã‚µãƒ¼ãƒ“ã‚¹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        drive_service = build("drive", "v3", credentials=creds)

        # ğŸ“Œ ãƒ•ã‚©ãƒ«ãƒ€IDã‚’ãã®ã¾ã¾æ¸¡ã™ï¼ˆget_drive_folder_id_by_path ã‚’ä½¿ã‚ãªã„ï¼‰
        process_all_sheets(spreadsheet_name, gc, drive_service, FOLDER_ID_MAIN, FOLDER_ID_BACKUP, creds)

    except Exception as e:
        print(f"âŒ é‡å¤§ã‚¨ãƒ©ãƒ¼: {e}")
        return

    elapsed = time.time() - start_time
    print("âœ… å…¨å‡¦ç†å®Œäº†ï¼")
    print(f"â±ï¸ æ‰€è¦æ™‚é–“: {elapsed:.1f} ç§’")

    # å„ã‚·ãƒ¼ãƒˆå˜ä½ã®ãƒ­ã‚°å‡ºåŠ›ï¼ˆçœç•¥ã›ãšå‡ºã™ï¼‰
    if sheet_process_logs:
        print("\nğŸ“Š ã‚·ãƒ¼ãƒˆã”ã¨ã®å‡¦ç†æ¦‚è¦:")
        for log in sheet_process_logs:
            print(f"\nğŸ“„ ã‚·ãƒ¼ãƒˆ: {log['sheet_name']}")
            print(f" - ä»¶æ•°: {log['count']} è¡Œ")
            print(f" - å‡¦ç†æ™‚é–“: {log['elapsed']:.1f} ç§’")
            if log['failures']:
                print(f" - âš ï¸ å¤±æ•—éŠ˜æŸ„: {len(log['failures'])} ä»¶")
                from collections import defaultdict
                grouped = defaultdict(list)
                for code in log['failures']:
                    try:
                        prefix = int(code) // 1000 * 1000
                        grouped[f"{prefix}ç•ªå°"].append(code)
                    except:
                        grouped["ä¸æ˜"].append(code)
                for group in sorted(grouped.keys()):
                    print(f"   ãƒ»{group}: [{', '.join(grouped[group])}]")
            else:
                print(" - ğŸ‰ å…¨éŠ˜æŸ„æ­£å¸¸ã«å–å¾—")
