# ================================
# Sec1ï½œã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
# ================================

!pip install --upgrade gspread gspread_dataframe google-auth yfinance --quiet
!pip install ta --quiet

import os
import pandas as pd
import yfinance as yf
import gspread
from google.colab import auth
from gspread_dataframe import get_as_dataframe, set_with_dataframe
from googleapiclient.discovery import build
from googleapiclient.http import MediaInMemoryUpload
from google.auth.transport.requests import Request
from datetime import datetime, timezone, timedelta
import google.auth
from google.colab import drive
import time
from collections import defaultdict

pd.set_option('future.no_silent_downcasting', True)

# ================================
# Sec2ï½œå®šç¾©ãƒ»é–¢æ•°
# ================================

import numpy as np

SAVE_PATH = "drive/MyDrive/ColabNotebooks/éŠ˜æŸ„åˆ†æ/signal"  # ğŸ”§ ä¿å­˜å…ˆã®ãƒ‘ã‚¹ã‚’æ­£ã—ãè¨­å®š
BACKUP_SUBFOLDER = "backup"  # ğŸ”§ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã¯ã“ã®ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€ã«

# GoogleDriveé–¢æ•°
def mount_drive():
    drive.mount('/content/drive')

def authenticate_services():
    auth.authenticate_user()
    creds, _ = google.auth.default()
    return creds, gspread.authorize(creds), creds

def get_drive_folder_id_by_path(path, creds):
    service = build('drive', 'v3', credentials=creds)
    parts = path.strip("/").split("/")
    parent_id = 'root'
    for part in parts:
        query = f"name='{part}' and mimeType='application/vnd.google-apps.folder' and '{parent_id}' in parents and trashed=false"
        results = service.files().list(q=query, spaces='drive', fields='files(id, name)').execute()
        files = results.get('files', [])
        if files:
            parent_id = files[0]['id']
        else:
            file_metadata = {
                'name': part,
                'mimeType': 'application/vnd.google-apps.folder',
                'parents': [parent_id]
            }
            file = service.files().create(body=file_metadata, fields='id').execute()
            parent_id = file['id']
    return parent_id, service

def delete_existing_file_by_name(drive_service, folder_id, file_name):
    query = f"'{folder_id}' in parents and name = '{file_name}' and mimeType = 'application/vnd.google-apps.spreadsheet'"
    results = drive_service.files().list(q=query, fields="files(id, name)").execute()
    files = results.get('files', [])
    for file in files:
        drive_service.files().delete(fileId=file['id']).execute()

def create_spreadsheet_in_folder(title, folder_id, creds):
    drive_service = build("drive", "v3", credentials=creds)
    file_metadata = {
        "name": title,
        "mimeType": "application/vnd.google-apps.spreadsheet",
        "parents": [folder_id]
    }
    file = drive_service.files().create(body=file_metadata, fields="id").execute()
    return file["id"]

# æ—¥ä»˜ã®å–å¾—
def get_today_str():
    JST = timezone(timedelta(hours=9))
    now = datetime.now(JST)
    return now.strftime("%Y-%m-%d"), now.strftime("%Y-%m-%d %H:%M:%S")

# ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®Colum
def reorder_columns(df, preferred_order):
    existing_cols = [col for col in preferred_order if col in df.columns]
    other_cols = [col for col in df.columns if col not in existing_cols]
    return df[existing_cols + other_cols]

def symbol_to_num(symbol):
    # .Tã‚’é™¤å»ã—ã€æ•°å€¤éƒ¨åˆ†ã®ã¿æŠ½å‡º
    s = str(symbol).replace('.T', '')
    try:
        return int(float(s))
    except ValueError:
        return np.nan  # æ•°å€¤åŒ–ã§ããªã„å ´åˆã¯NaN

def clean_dataframe_columns(df, expected_cols):
    """
    DataFrameã‹ã‚‰é‡è¤‡åˆ—ï¼ˆä¾‹: 'MultiSign.1'ï¼‰ã‚’å–ã‚Šé™¤ãã€åˆ—é †ã‚’expected_colsã«æ•´ãˆã‚‹ã€‚
    """
    # 1. æœŸå¾…ã•ã‚Œã‚‹åˆ—åã«å¯¾ã—ã¦ã€.1 ã‚„ .2 ãŒã¤ã„ãŸé‡è¤‡åˆ—ãŒã‚ã‚Œã°å‰Šé™¤
    deduped_cols = {}
    for col in df.columns:
        base_col = col.split('.')[0]
        if base_col not in deduped_cols:
            deduped_cols[base_col] = col  # æœ€åˆã«å‡ºã¦ããŸæ­£è¦åã‚’æ¡ç”¨

    df = df[list(deduped_cols.values())]

    # 2. åˆ—é †ã‚’expected_colsã®é †ã«æ•´ãˆã‚‹ï¼ˆå­˜åœ¨ã™ã‚‹åˆ—ã®ã¿ï¼‰
    df = df[[col for col in expected_cols if col in df.columns]]

    return df

def normalize_symbol_column(df):
    """
    Symbolåˆ—ã‚’ "XXXX.T" å½¢å¼ã«çµ±ä¸€ã™ã‚‹
    - floatå½¢å¼ã® .0 é™¤å»
    - .T ã®é‡è¤‡é™¤å»
    - .0.T ã®èª¤å¤‰æ›é™¤å»
    """
    def clean_symbol(val):
        try:
            val = str(val).strip().replace('$', '')
            val = val.replace('.0.T', '')  # â† è¿½åŠ ãƒã‚¤ãƒ³ãƒˆ
            if val.endswith('.0'):
                val = val[:-2]
            val = val.replace('.T', '')  # é‡è¤‡é™¤å»
            return val + '.T'
        except:
            return ""

    if "Symbol" in df.columns:
        df["Symbol"] = df["Symbol"].apply(clean_symbol)
    return df

import pandas as pd
import numpy as np
from ta.trend import ADXIndicator, MACD
from ta.momentum import RSIIndicator

def detect_candlestick_patterns(df):
    """ãƒ­ãƒ¼ã‚½ã‚¯è¶³ã®å½¢çŠ¶ã«åŸºã¥ã„ã¦ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡ºã™ã‚‹"""
    patterns = []

    open_ = df["Open"].iloc[-1]
    close = df["Close"].iloc[-1]
    high = df["High"].iloc[-1]
    low = df["Low"].iloc[-1]

    body = abs(close - open_)
    upper_shadow = high - max(open_, close)
    lower_shadow = min(open_, close) - low

    # ä¸‹é«­é™½ç·šï¼šé™½ç·šã‹ã¤é•·ã„ä¸‹ãƒ’ã‚²
    if close > open_ and lower_shadow > body * 2:
        patterns.append("ä¸‹é«­é™½ç·š")

    # é™½ã®ä¸¸åŠä¸»ï¼šé™½ç·šã§ä¸Šä¸‹ãƒ’ã‚²ãŒéå¸¸ã«çŸ­ã„
    if close > open_ and upper_shadow < body * 0.1 and lower_shadow < body * 0.1:
        patterns.append("é™½ã®ä¸¸åŠä¸»")

    return patterns

def detect_multi_candle_patterns(df):
    """è¤‡æ•°æ—¥ã®ãƒ­ãƒ¼ã‚½ã‚¯è¶³å½¢çŠ¶ã«åŸºã¥ããƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º"""
    patterns = []

    if len(df) < 5:
        return patterns  # éå»3æ—¥å¿…è¦

    # ç›´è¿‘3æ—¥ã®ãƒ‡ãƒ¼ã‚¿
    last3 = df.iloc[-3:]
    open_ = last3["Open"]
    close = last3["Close"]
    high = last3["High"]
    low = last3["Low"]

    # ä¸‰é€£ç¶šé™½ç·š
    if all(close.values > open_.values):
        patterns.append("ä¸‰é€£ç¶šé™½ç·š")

    # ä¸‰ç©ºè¸ã¿ä¸Šã’ï¼ˆé€£ç¶šã§ä¸Šæ–¹å‘ã«çª“ã‚’é–‹ã‘ã¦å§‹ã¾ã‚‹ï¼‰
    gaps = [
        open_.iloc[1] > high.iloc[0],
        open_.iloc[2] > high.iloc[1]
    ]
    if all(gaps):
        patterns.append("ä¸‰ç©ºè¸ã¿ä¸Šã’")

    return patterns

def is_ath_breakout(df):
    """çµ‚å€¤ãŒä¸Šå ´æ¥é«˜å€¤ä»˜è¿‘ã«ã‚ã‚‹ã‹åˆ¤å®š"""
    recent_high = df["High"].max()
    return df["Close"].iloc[-1] >= recent_high * 0.995

def analyze_stock(df):
    try:
        if len(df) < 30:
            raise ValueError("ãƒ‡ãƒ¼ã‚¿ä¸è¶³")

        df = df.copy()
        close = df["Close"]
        open_ = df["Open"]
        high = df["High"]
        low = df["Low"]
        volume = df["Volume"]

        df["MA5"] = close.rolling(5).mean()
        df["MA25"] = close.rolling(25).mean()
        df["RSI"] = RSIIndicator(close, window=14).rsi()
        macd = MACD(close)
        df["MACD"] = macd.macd()
        df["MACD_signal"] = macd.macd_signal()
        adx = ADXIndicator(high, low, close, window=14)
        df["ADX"] = adx.adx()

        rsi_last = df["RSI"].iloc[-1]
        adx_last = df["ADX"].iloc[-1]
        close_now = close.iloc[-1]
        close_prev = close.iloc[-2]
        open_now = open_.iloc[-1]
        high_prev = high.iloc[-2]
        low_prev = low.iloc[-2]
        ma5_prev = df["MA5"].iloc[-2]
        ma25_prev = df["MA25"].iloc[-2]
        ma5_now = df["MA5"].iloc[-1]
        ma25_now = df["MA25"].iloc[-1]
        ma25 = ma25_now

        disparity = ((close_now - ma25) / ma25) * 100 if ma25 else 0

        signals = []

        # === ã‚·ã‚°ãƒŠãƒ«æ¤œå‡º ===
        if df["MACD"].iloc[-2] < df["MACD_signal"].iloc[-2] and df["MACD"].iloc[-1] > df["MACD_signal"].iloc[-1]:
            signals.append("MACDé™½è»¢")
        if df["MACD"].iloc[-2] > df["MACD_signal"].iloc[-2] and df["MACD"].iloc[-1] < df["MACD_signal"].iloc[-1]:
            signals.append("MACDé™°è»¢")

        if ma5_prev < ma25_prev and ma5_now > ma25_now:
            signals.append("çŸ­æœŸã‚´ãƒ¼ãƒ«ãƒ‡ãƒ³ã‚¯ãƒ­ã‚¹")
        if ma5_prev > ma25_prev and ma5_now < ma25_now:
            signals.append("çŸ­æœŸãƒ‡ãƒƒãƒ‰ã‚¯ãƒ­ã‚¹")

        if df["RSI"].iloc[-2] < 30 and rsi_last > 30:
            signals.append("RSIåç™º")
        if df["RSI"].iloc[-2] > 70 and rsi_last < 70:
            signals.append("RSIéç†±åè»¢")

        recent_high = df["High"].iloc[-20:-1].max()
        if close_now > recent_high:
            signals.append("ç›´è¿‘é«˜å€¤çªç ´")
        elif close_now < recent_high * 0.98:
            signals.append("é«˜å€¤åè½")

        if volume.iloc[-1] > volume.rolling(5).mean().iloc[-2] * 1.5:
            signals.append("å‡ºæ¥é«˜â†‘")

        if open_now > high_prev:
            signals.append("çª“é–‹ã‘ä¸Šæ˜‡")
        elif open_now < low_prev:
            signals.append("çª“é–‹ã‘ä¸‹è½")

        pct_change = ((close_now - close_prev) / close_prev) * 100
        if pct_change >= 7:
            signals.append("æ€¥é¨°")
        elif pct_change <= -7:
            signals.append("æ€¥è½")

        if close_now >= df["High"].max() * 0.995:
            signals.append("ä¸Šå ´æ¥é«˜å€¤çªç ´")

        signals.extend(detect_candlestick_patterns(df))
        signals.extend(detect_multi_candle_patterns(df))

        # â­ æ³¨ç›®åº¦åˆ¤å®šï¼ˆã‚¹ã‚³ã‚¢ã§ã¯ãªãsignalsã«åŸºã¥ãï¼‰
        def judge_attention(signals):
            keywords = {
                "â˜…â˜…â˜…": ["ä¸‰ç©ºè¸ã¿ä¸Šã’", "ä¸Šå ´æ¥é«˜å€¤", "ç›´è¿‘é«˜å€¤çªç ´"],
                "â˜…â˜…": ["ä¸‰é€£ç¶šé™½ç·š", "MACDé™½è»¢", "çŸ­æœŸã‚´ãƒ¼ãƒ«ãƒ‡ãƒ³ã‚¯ãƒ­ã‚¹", "å‡ºæ¥é«˜â†‘"],
                "â˜…": ["RSIåç™º", "çª“é–‹ã‘ä¸Šæ˜‡"]
            }
            for star, keys in keywords.items():
                if any(k in signals for k in keys):
                    return star
            return ""

        attention_flag = judge_attention(signals)

        # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†é¡ï¼ˆã‚¹ã‚³ã‚¢ã«é–¢ä¿‚ãªãæ®‹ã—ã¦OKï¼‰
        if adx_last >= 40:
            trend = "å¼·ãƒˆãƒ¬ãƒ³ãƒ‰"
        elif adx_last >= 25:
            trend = "ä¸­ãƒˆãƒ¬ãƒ³ãƒ‰"
        else:
            trend = "å¼±ãƒˆãƒ¬ãƒ³ãƒ‰"

        return signals, adx_last, trend, rsi_last, disparity, attention_flag

    except Exception as e:
        print(f"âŒ analyze_stock ã‚¨ãƒ©ãƒ¼: {e}")
        return [], None, "ä¸æ˜", 0.0, None, ""

# Commentåˆ¤å®š
# åˆ†æåˆ¤å®š@2025-06-11.å£²ã‚Šåˆ¤å®šã‚‚è¿½åŠ 
def format_signals_to_comment(signals, adx_value=None):
    phrases = []
    warnings = []

    # ===== è²·ã„å‚¾å‘ã‚³ãƒ¡ãƒ³ãƒˆ =====
    if "MACDé™½è»¢" in signals:
        phrases.append("MACDãŒé™½è»¢ã—ã€è²·ã„ã®å‹¢ã„ãŒå‡ºå§‹ã‚ã¦ã„ã¾ã™ã€‚")
    if "çŸ­æœŸã‚´ãƒ¼ãƒ«ãƒ‡ãƒ³ã‚¯ãƒ­ã‚¹" in signals:
        phrases.append("çŸ­æœŸã®ç§»å‹•å¹³å‡ç·šãŒã‚´ãƒ¼ãƒ«ãƒ‡ãƒ³ã‚¯ãƒ­ã‚¹ã‚’å½¢æˆã—ã¦ã„ã¾ã™ã€‚")
    if "RSIåç™º" in signals:
        phrases.append("RSIãŒåç™ºã—ã€åè»¢ä¸Šæ˜‡ã®å…†ã—ãŒã‚ã‚Šã¾ã™ã€‚")
    if "å‡ºæ¥é«˜â†‘" in signals:
        phrases.append("å‡ºæ¥é«˜ãŒæ€¥å¢—ã—ã¦ãŠã‚Šæ³¨ç›®ãŒé›†ã¾ã£ã¦ã„ã¾ã™ã€‚")
    if "ç›´è¿‘é«˜å€¤çªç ´" in signals:
        phrases.append("ç›´è¿‘ã®é«˜å€¤ã‚’ãƒ–ãƒ¬ã‚¤ã‚¯ã—ã€ãƒˆãƒ¬ãƒ³ãƒ‰ç¶™ç¶šãŒæœŸå¾…ã•ã‚Œã¾ã™ã€‚")
    if "ä¸‰é€£ç¶šé™½ç·š" in signals:
        phrases.append("3æ—¥é€£ç¶šã®é™½ç·šãŒç¶šã„ã¦ãŠã‚Šã€å¼·ã„ä¸Šæ˜‡åœ§åŠ›ãŒç¢ºèªã§ãã¾ã™ã€‚")
    if "ä¸‰ç©ºè¸ã¿ä¸Šã’" in signals:
        phrases.append("ä¸‰ç©ºè¸ã¿ä¸Šã’ãŒå‡ºç¾ã—ã¦ãŠã‚Šã€å¼·ã„è²·ã„ç›¸å ´ã®ç¶™ç¶šãŒç¤ºå”†ã•ã‚Œã¾ã™ã€‚")

    # ===== å£²ã‚Šãƒ»æ³¨æ„ã‚·ã‚°ãƒŠãƒ«ã‚³ãƒ¡ãƒ³ãƒˆ =====
    if "MACDé™°è»¢" in signals:
        warnings.append("MACDãŒé™°è»¢ã—ã€ä¸‹è½è»¢æ›ã®å…†ã—ãŒã‚ã‚Šã¾ã™ã€‚")
    if "çŸ­æœŸãƒ‡ãƒƒãƒ‰ã‚¯ãƒ­ã‚¹" in signals:
        warnings.append("çŸ­æœŸãƒ‡ãƒƒãƒ‰ã‚¯ãƒ­ã‚¹ãŒç™ºç”Ÿã—ã€ä¸‹è½ãƒªã‚¹ã‚¯ã«æ³¨æ„ãŒå¿…è¦ã§ã™ã€‚")
    if "RSIéç†±åè»¢" in signals:
        warnings.append("RSIãŒéç†±åœã‹ã‚‰åè»¢ã—ã¦ãŠã‚Šã€åˆ©ç¢ºã‚„èª¿æ•´ã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚")
    if "é«˜å€¤åè½" in signals:
        warnings.append("é«˜å€¤æ›´æ–°ã«å¤±æ•—ã—ã¦ãŠã‚Šã€åè½ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
    if "ä¸‰ç©ºè¸ã¿ä¸Šã’" in signals:
        warnings.append("ãŸã ã—ä¸‰ç©ºè¸ã¿ä¸Šã’ã¯éç†±ã®å…†å€™ã¨ã‚‚ã•ã‚Œã€æŠ¼ã—ç›®ã‚„åè½ã«ã¯è­¦æˆ’ãŒå¿…è¦ã§ã™ã€‚")

    # ===== ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦ï¼ˆADXï¼‰ã‚³ãƒ¡ãƒ³ãƒˆ =====
    if adx_value is not None:
        if adx_value >= 40:
            phrases.append("ãƒˆãƒ¬ãƒ³ãƒ‰ã¯éå¸¸ã«å¼·ãã€æµã‚Œã«ä¹—ã‚‹æˆ¦ç•¥ãŒæœ‰åŠ¹ã§ã™ã€‚")
        elif adx_value >= 25:
            phrases.append("ä¸­ç¨‹åº¦ã®ãƒˆãƒ¬ãƒ³ãƒ‰ãŒç¶™ç¶šã—ã¦ã„ã¾ã™ã€‚")

    # ===== ã‚³ãƒ¡ãƒ³ãƒˆæ–‡ç”Ÿæˆ =====
    final_comment = " ".join(phrases + warnings)
    return final_comment

def judge_action_by_comment(comment: str) -> str:
    """
    ã‚³ãƒ¡ãƒ³ãƒˆå†…å®¹ã‹ã‚‰ã€Actionåˆ—ï¼ˆè²·ã„/ä¸­ç«‹/å£²ã‚Šï¼‰ã‚’åˆ¤å®šã™ã‚‹
    """
    buy_keywords = ["è²·ã„", "åç™º", "ä¸Šæ˜‡", "ãƒ–ãƒ¬ã‚¤ã‚¯", "é™½ç·š", "æ³¨ç›®"]
    sell_keywords = ["ãƒ‡ãƒƒãƒ‰ã‚¯ãƒ­ã‚¹", "é™°è»¢", "ä¸‹è½", "éç†±", "åè½", "åˆ©ç¢º", "è­¦æˆ’"]

    comment_lower = comment.lower()

    buy_score = sum(1 for k in buy_keywords if k in comment)
    sell_score = sum(1 for k in sell_keywords if k in comment)

    if buy_score > 1 and sell_score == 0:
        return "è²·ã„ï¼ˆã‚¨ãƒ³ãƒˆãƒªãƒ¼æ¤œè¨ï¼‰"
    elif sell_score > 1 and buy_score == 0:
        return "å£²ã‚Šï¼ˆåˆ©ç¢º/èª¿æ•´ï¼‰"
    else:
        return "ä¸­ç«‹ï¼ˆæ§˜å­è¦‹ï¼‰"

def process_all_sheets(spreadsheet_name, gc, drive_service, folder_id_main, folder_id_backup, creds):
    sheet_process_logs = []
    global SAVE_PATH
    today_str, timestamp_str = get_today_str()
    spreadsheet = gc.open(spreadsheet_name)
    sheet_list = spreadsheet.worksheets()

    all_dfs = []  # å„ã‚·ãƒ¼ãƒˆã®DFã‚’æ ¼ç´

    for worksheet in sheet_list:
        start_time_sheet = time.time()
        sheet_name = worksheet.title
        print(f"\U0001f50d å‡¦ç†ä¸­: {sheet_name}")
        df = get_as_dataframe(worksheet, evaluate_formulas=True)

        if "Symbol" not in df.columns:
            print(f"âš ï¸ {sheet_name} ã«Symbolåˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            continue

        for col in ["Name", "Time", "Price", "Sign", "MultiSign", "Action", "æ³¨ç›®åº¦"]:
            if col not in df.columns:
                df[col] = pd.Series([""] * len(df), dtype="object")
            else:
                df[col] = df[col].astype("object")

        df["Time"] = df["Time"].astype(str)

        for i, row in df.iterrows():
            if i % 100 == 0:
                print(f"   ğŸ”„ {i+1}ä»¶ç›®ã‚’å‡¦ç†ä¸­...")
            raw_symbol = None
            try:
                raw_symbol = str(row["Symbol"]).strip().replace("$", "").replace(".T", "")
                if "." in raw_symbol:
                    raw_symbol = raw_symbol.split(".")[0]
                code = raw_symbol.zfill(4)
                ticker = f"{code}.T"

                stock = yf.Ticker(ticker)
                hist = stock.history(period="3mo")
                if hist.empty or len(hist) < 30:
                    print(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿ä¸è¶³: {ticker} - ãƒ’ã‚¹ãƒˆãƒªã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿{len(hist)}ä»¶")
                    continue

                df.at[i, "Price"] = round(hist["Close"].iloc[-1], 2)
                if pd.isnull(row["Name"]) or row["Name"] == "":
                    df.at[i, "Name"] = stock.info.get("shortName", "")

                signals, adx_last, trend, rsi_last, disparity, attention_flag = analyze_stock(hist)
                df.at[i, "Sign"] = "âœ… " + ", ".join(signals) if signals else ""
                comment = format_signals_to_comment(signals, adx_last)
                df.at[i, "MultiSign"] = comment
                df.at[i, "Action"] = judge_action_by_comment(comment)
                df.at[i, "Time"] = timestamp_str
                df.at[i, "æ³¨ç›®åº¦"] = attention_flag

            except Exception as e:
                print(f"âš ï¸ ã‚¨ãƒ©ãƒ¼: {raw_symbol or 'ä¸æ˜'} - {e}")

        # Symbolåˆ—ã‚’æ­£è¦åŒ–ã™ã‚‹
        df = normalize_symbol_column(df)

        df = reorder_columns(df, ["Symbol", "Name", "Time", "Price", "æ³¨ç›®åº¦", "Action", "Sign", "MultiSign"])
        df = clean_dataframe_columns(df, ["Symbol", "Name", "Time", "Price", "æ³¨ç›®åº¦", "Action", "Sign", "MultiSign"])

        all_dfs.append(df.copy())

        df.drop(columns=[col for col in ["TrendScore", "TrendScore_raw"] if col in df.columns], inplace=True)
        worksheet.clear()
        set_with_dataframe(worksheet, df.fillna("").infer_objects(copy=False))

        new_sheet_title = f"Watchlist_{sheet_name}_{today_str}"
        try:
            delete_existing_file_by_name(drive_service, folder_id_backup, new_sheet_title)
            new_spreadsheet = gc.create(new_sheet_title, folder_id_backup)
            set_with_dataframe(new_spreadsheet.sheet1, df.fillna("").infer_objects(copy=False))
            print(f"âœ… å®Œäº†: {new_sheet_title}")
        except Exception as e:
            print(f"âŒ ä½œæˆå¤±æ•—: {new_sheet_title} - {e}")

        elapsed_sheet = time.time() - start_time_sheet
        sheet_process_logs.append({
            "sheet_name": sheet_name,
            "count": len(df),
            "elapsed": elapsed_sheet,
            "failures": [row["Symbol"] for i, row in df.iterrows() if "ã‚¨ãƒ©ãƒ¼" in str(row["MultiSign"]) or "å–å¾—å¤±æ•—" in str(row["Name"])]
        })

    # âœ… æ¡ä»¶ä¸€è‡´éŠ˜æŸ„ã®çµ±åˆã¨å‡ºåŠ›
    try:
        combined_df = pd.concat(all_dfs, ignore_index=True)

        # âœ… Signalæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        combined_df["SignalCount"] = combined_df["Sign"].apply(
            lambda x: x.count(",") + 1 if isinstance(x, str) and x.startswith("âœ…") else 0
        )

        # âœ… æ¡ä»¶ä¸€è‡´: Actionã¾ãŸã¯æ³¨ç›®åº¦ + SignalãŒ3å€‹ä»¥ä¸Š
        condition = (
            ((combined_df["Action"] == "è²·ã„ï¼ˆã‚¨ãƒ³ãƒˆãƒªãƒ¼æ¤œè¨ï¼‰") | (combined_df["æ³¨ç›®åº¦"] == "â˜…â˜…â˜…"))
            & (combined_df["SignalCount"] >= 3)
        )
        filtered_df = combined_df[condition].copy()
        filtered_df.drop(columns=["SignalCount"], inplace=True)

        # âœ… ã‚½ãƒ¼ãƒˆã¨å‡ºåŠ›å‡¦ç†
        filtered_df["SortKey"] = filtered_df["Symbol"].apply(symbol_to_num)
        filtered_df.sort_values("SortKey", inplace=True)
        filtered_df.drop(columns=["SortKey"], inplace=True)

        summary_title = f"watchlist_signal_{today_str}"
        delete_existing_file_by_name(drive_service, folder_id_main, summary_title)
        summary_sheet_id = create_spreadsheet_in_folder(summary_title, folder_id_main, creds)
        summary_sheet = gc.open_by_key(summary_sheet_id)
        set_with_dataframe(summary_sheet.sheet1, filtered_df.fillna("").infer_objects(copy=False))

        sheet_url = f"https://docs.google.com/spreadsheets/d/{summary_sheet_id}/edit"
        print(f"ğŸ“¤ æ¡ä»¶ä¸€è‡´ã‚·ã‚°ãƒŠãƒ«å‡ºåŠ›å®Œäº†: {summary_title}")
        print(f"ğŸ”— å‡ºåŠ›å…ˆURL: {sheet_url}")

    except Exception as e:
        print(f"âš ï¸ æ¡ä»¶ä¸€è‡´ã®å‡ºåŠ›å¤±æ•—: {e}")

    # === ãƒ‡ãƒãƒƒã‚°å‡ºåŠ› ===
    print("\nğŸ”¬ ãƒ‡ãƒãƒƒã‚°æƒ…å ±:")
    print(f"å…¨ã‚·ãƒ¼ãƒˆæ•°: {len(sheet_list)}")
    print(f"å‡¦ç†æ¸ˆã¿ã‚·ãƒ¼ãƒˆæ•°: {len(all_dfs)}")
    print(f"å…¨ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(combined_df)}")
    print(f"ãƒ•ã‚£ãƒ«ã‚¿å¾Œä»¶æ•°: {len(filtered_df)}")
    print(f"ğŸ“ ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€IDï¼ˆmainï¼‰: {folder_id_main}")
    print("ğŸ‰ ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ")

def create_spreadsheet_in_folder(title, folder_id, creds):
    drive_service = build("drive", "v3", credentials=creds)
    file_metadata = {
        "name": title,
        "mimeType": "application/vnd.google-apps.spreadsheet",
        "parents": [folder_id]
    }
    file = drive_service.files().create(body=file_metadata, fields="id").execute()
    return file["id"]


# ================================
# âœ… ãƒ¡ã‚¤ãƒ³å‡¦ç†é–¢æ•°
# ================================

def main():
    # ã‚ãªãŸã®Driveä¸Šã®ç›®çš„ãƒ•ã‚©ãƒ«ãƒ€IDã«å·®ã—æ›¿ãˆã¦ãã ã•ã„
    FOLDER_ID_MAIN = "1dFuJfNLSJ7tw43Ac9RKAqJ0yW49cLsIe"       # ColabNotebooks/éŠ˜æŸ„åˆ†æ/Watchlist
    FOLDER_ID_BACKUP = "1hN6fzMeT1ZB7I0jVRc9L_8HKIqI0Gi_W"     # ColabNotebooks/éŠ˜æŸ„åˆ†æ/Watchlist/backup

    spreadsheet_name = "watchlist"
    mount_drive()

    start_time = time.time()
    print("ğŸš€ å‡¦ç†é–‹å§‹")

    global sheet_process_logs
    sheet_process_logs = []  # â† ãƒ­ã‚°åˆæœŸåŒ–

    try:
        _, gc, creds = authenticate_services()

        # Google Drive APIã®ã‚µãƒ¼ãƒ“ã‚¹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        drive_service = build("drive", "v3", credentials=creds)

        # ğŸ“Œ ãƒ•ã‚©ãƒ«ãƒ€IDã‚’ãã®ã¾ã¾æ¸¡ã™ï¼ˆget_drive_folder_id_by_path ã‚’ä½¿ã‚ãªã„ï¼‰
        process_all_sheets(spreadsheet_name, gc, drive_service, FOLDER_ID_MAIN, FOLDER_ID_BACKUP, creds)

    except Exception as e:
        print(f"âŒ é‡å¤§ã‚¨ãƒ©ãƒ¼: {e}")
        return

    elapsed = time.time() - start_time
    print("âœ… å…¨å‡¦ç†å®Œäº†ï¼")
    print(f"â±ï¸ æ‰€è¦æ™‚é–“: {elapsed:.1f} ç§’")

    # å„ã‚·ãƒ¼ãƒˆå˜ä½ã®ãƒ­ã‚°å‡ºåŠ›ï¼ˆçœç•¥ã›ãšå‡ºã™ï¼‰
    if sheet_process_logs:
        print("\nğŸ“Š ã‚·ãƒ¼ãƒˆã”ã¨ã®å‡¦ç†æ¦‚è¦:")
        for log in sheet_process_logs:
            print(f"\nğŸ“„ ã‚·ãƒ¼ãƒˆ: {log['sheet_name']}")
            print(f" - ä»¶æ•°: {log['count']} è¡Œ")
            print(f" - å‡¦ç†æ™‚é–“: {log['elapsed']:.1f} ç§’")
            if log['failures']:
                print(f" - âš ï¸ å¤±æ•—éŠ˜æŸ„: {len(log['failures'])} ä»¶")
                from collections import defaultdict
                grouped = defaultdict(list)
                for code in log['failures']:
                    try:
                        prefix = int(code) // 1000 * 1000
                        grouped[f"{prefix}ç•ªå°"].append(code)
                    except:
                        grouped["ä¸æ˜"].append(code)
                for group in sorted(grouped.keys()):
                    print(f"   ãƒ»{group}: [{', '.join(grouped[group])}]")
            else:
                print(" - ğŸ‰ å…¨éŠ˜æŸ„æ­£å¸¸ã«å–å¾—")
